<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>概统 | My Blog</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="概统" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="About 挺一般的研究生课程 评价是纯纯的 PPT 朗诵. Memonic 贝叶斯公式 \(P(B_{i} | A) = \frac{P(A|B_{i})P(B_{i})}{&sum;_{j=1}^{n} P(A|B_{j}) P(B_{j})}\) 随机变量映射 \(F_{Y}(Y) = P(g(X) \leq Y)\) 统计量数字特征计算 特征随机变量子样 平均\(\frac{1}{n} &sum; x_i\)\(\frac{1}{n} &sum; x_{i}\) 方差\(&sigma;^{2} = &sum; (X - \bar{X})^{2} = &sum; X^{2} - (&sum; X)^{2}\)\(S^{2} = \frac{1}{n-1} &sum; (x_{i} - \bar{x}_{i})^{2} = \frac{1}{n-1} ((&sum; x_{i}^{2}) - n \bar{x}^{2})\) 多元方差\(V(&sum; a_{i} X_{i}) = &sum; a_{i}^{2} V(X_{i}) + &sum; a_{i} a_{j} \mathrm{cov}(X_{i}, X_{j})\)- 协方差\(\mathrm{cov}(X, Y) = E((X - E(X))(Y - E(Y))) = \mathrm{cov}(X, Y) = E(XY) - E(X)E(Y)\)\(S_{XY} = \frac{1}{n-1} &sum; (x_{i} - \bar{x})(y_{i} - \bar{y})\) 相关系数\(&rho;_{XY} = \mathrm{cov}(X, Y) / (&sigma;_{X} &sigma;_{Y})\)\(r = \frac{&sum; (x_{i} - \bar{x})(y_{i} - \bar{y})}{\sqrt{(x_{i} - \bar{x})} \sqrt{(y_{i} - \bar{y})}}\) 实验数据合并 \[&mu; = \frac{&sum; \frac{&mu;_{i}}{&sigma;_{i}^{2}}}{ &sum; \frac{1}{ &sigma;_{i}^{2 }} },\ V = \frac{1}{&sum; \frac{1}{&sigma;_{i}^{2} }}\] 极大似然法 \[L = &prod; f(x_{i}|&theta;),\ &part;_{&theta;} ln L = 0 &and; &part;_{&theta;}^{2} ln L &lt; 0\] 最小二乘法 \[\mathrm{err}(&theta;) = &sum; (y_{i} - f(x_{i}|&theta;))^{2},\ &part;_{&theta;} \mathrm{err} = 0 &and; &part;_{&theta;}^{2} \mathrm{err} &gt; 0\] 假设检验 条件检验对象统计量观测值拒绝域 \(&sigma;^{2}\) 已知均值 \(&mu;\)\(Z = \frac{\bar{X} - &mu;_{0}}{&sigma; / \sqrt{n}}\)\(\left\vert Z \right\vert &gt; Z_{&alpha;/2}(n-1)\) \(&sigma;^{2}\) 未知均质 \(&mu;\)\(t = \frac{\bar{X} - &mu;_{0}}{S / \sqrt{n}}\)\(\left\vert t \right\vert &gt; t_{&alpha;/2}(n-1)\) 方差 \(&sigma;^{2}\)\(&chi;^{2} = \frac{(n-1) S^{2}}{&sigma;_{0}^{2}}\)\(&chi;^{2} &lt; &chi;_{1-&alpha;/2}^{2}(n-1) &or; &chi;^{2} &gt; &chi;_{&alpha;/2}^{2}(n-1)\) 几个分布 分布PDFE(X)V(X)Notes 二项分布 \(B(n, p)\)\(\left(\begin{matrix} n &#92;&#92; k\end{matrix}\right) p^{k} (1-p)^{n-k}\)\(np\)\(np(1-p)\)(多项式分布) 泊松分布 \(P(&lambda;)\)\(\frac{&lambda;^{k} \mathrm{e}^{-&lambda;}}{k!}\)\(&lambda;\)\(&lambda;\)粒子计数标准差 \(\sqrt{N}\) 正态分布 \(N(&mu;,&sigma;^{2})\)\(\frac{1}{\sqrt{2 &pi;} &sigma;} \mathrm{exp}\left( - \frac{(x-&mu;)^{2}}{2&sigma;^{2}} \right)\)\(&mu;\)\(&sigma;\) 概率论初步 随机试验, 随机事件, 样本空间 GPT: 用一句话解释名词概念 必然现象: 在特定条件下必定发生的现象, 任何情况下都会发生 随机现象: 其结果不可预知, 受随机因素影响的现象 随机试验: 可以重复进行且结果不确定的实验或过程 互相独立: 两个或多个事件的发生与否互不影响 随机事件: 随机试验中可能发生的结果或现象 基本事件: 在随机试验中不再可以分解的最基本的事件 样本空间: 随机试验中所有可能基本事件的集合 元素: 样本空间中的每一个基本事件 必然事件: 一定会发生的事件, 其概率为 1 不可能事件: 一定不会发生的事件, 其概率为 0 事件关系 事件运算 \(A &and; B\), \(A &or; B\), \(&not; A\) 概率 Application 多项式分布 盒子里有 5 个红球, 4 个白球和 3 个蓝球, 从盒子里随机选一球, 记下颜色, 然后返回盒子里. 求挑选 6 次后出现 3 红, 2 白, 1 蓝的概率. 有放回取样 (多项式分布): \[P(\left\{ p_{i}, n_{i} \right\}) = \frac{(&sum; n_{i})!}{&prod; n_{i}!} &prod; p_{i}^{n_{i}}\] 无放回取样 (超几何分布): \[P(\left\{ p_{i}, n_{i} \right\}) = \frac{&prod; \left(\begin{matrix} N_{i} &#92;&#92; n_{i} \end{matrix}\right)}{\left(\begin{matrix} &sum; N_{i} &#92;&#92; &sum; n_{i} \end{matrix}\right)}\] Definition: \(\left(\begin{matrix} N &#92;&#92; n \end{matrix}\right) = \frac{N &times; (N - 1) &times; \cdots &times; (n+1)}{n &times; \cdots &times; 1}\) 条件概率 \(P(B|A) = \frac{P(A &and; B)}{P(A)}\) 事件独立性 Definition: \(P(A B) = P(A) P(B) &hArr; P(B|A) = P(B) &hArr;\) A, B 独立 Application 判断随机变量的独立性 设随机变量 \(X, Y\) 的 PDF 为 \(f(x, y) = A \mathrm{e}^{- a x^2 + b x y - c y^2}, - &infin; &lt; x &lt; &infin;, - &infin; &lt; y &lt; &infin;\), 问在什么条件下 \(X\) 与 \(Y\) 相互独立? 证明需要证明 \(f(x, y) = f_{X}(x) f_{Y}(y)\) 概率计算 边沿概率, 全概率公式, 贝叶斯公式 边沿概率 Application: 已知联合分布, 计算边缘分布 补全联合分布 全概率公式 \(P(A) = &sum; P(A | B_{i}) P(B_{i})\), 其中 \(\left\{ B_{i} \right\}\) 为样本空间的划分 贝叶斯公式 \(P(B_{i} | A) = \frac{P(A|B_{i})P(B_{i})}{&sum;_{j=1}^{n} P(A|B_{j}) P(B_{j})}\) Memonic: \(P(B_{i} | A) P(A) = P(B_{i} &and; A) = P(A | B_{i}) P(B_{i})\) Application: 误判概率计算 (逆概率计算) 已知某放射源产生的射线中包含 A, B, C 三种粒子, 占比分别 1/2, 1/6, 1/3. 在实验的粒子鉴别过程中, A 粒子被误判为其他粒子的概率为 10%, B, C 粒子被误判为 A 粒子的概率分别为 12% 和 21%. 请计算出被鉴别为 A 粒子的事例来自真实的 A 粒子的概率. 已知 \(P(S_{A})\) (\(S_{*}\) 对样本空间的划分) 以及 \(P(D_{A} | S_{ *})\) 求 \(P(S_{A} | D_{A}) = \frac{P(D_{A} | S_{A}) P(S_{A})}{&sum; P(D_{A}|S_{})P(S_{})\) 随机变量及其分布 随机变量 为了和之后的子样 (\(x_{i}\)) 作区分, 这里用 \(X\) 表示随机变量 数学符号一坨乱麻 不知道是不是 PPT 里面的数学公式和文本是东拼西凑的, 里面的数学公式符号非常的不一致&#8230; 学起来很混乱. 概率密度函数 (PDF): \(f(X)\) 累计分布函数 (CDF): \(F(X) = &int; f(X) \mathrm{d}x\) 随机变量的映射: Algorithm I: \(F_{y}(y) = P(g(x) \leq y)\) Algorithm II: \(g\) 为映射作用在随机变量 \(X\) 上, 则 \(f_{Y}(Y) = f_{X}(g^{-1}(Y)) \left| \frac{\mathrm{d}}{\mathrm{d} Y} g^{-1}(Y) \right|\). Memonic: \(F_{Y}(Y) = &int; f_{Y}(Y) \mathrm{d} Y = &int; f_{X}(X) \mathrm{d} X \frac{\mathrm{d} X}{\mathrm{d} Y} &rArr; f_{Y} = f_{X}(g^{-1}(Y)) \frac{\mathrm{d} X}{\mathrm{d} Y}\) Application: 计算概率密度函数映射 随机变量 \(X\) 的概率密度函数为: \[f(x) = \left\{ \begin{matrix} x^2 / 18 &amp; -3 &lt; x &lt; 3 &#92;&#92; 0 &amp; \mathrm{other} \end{matrix} \right.\] 求 \(Y = (X + 1)^2\) 的概率密度函数. 非单调映射 \(F_{Y}(y) = P(g(X) \leq y)\) 计算变换关系 已知 \(X, Y\) 为 \((0, 1)\) 区间上均匀分布的随机变量且相互独立, 二维随机变量 \(U, V\) 和 \(X, Y\) 之间存在变换关系: \(U = cos (2 &pi; x) \sqrt{- 2 ln y}, V = sin (2 &pi; x) \sqrt{- 2 ln y}\), 证明 \(U, V\) 服从 \(N(0, 1)\). 映射的数字特征 随机变量 \(X, Y\) 都服从 \(N(&mu;, &sigma;^2)\) 且相互独立, 令 \(U = a X + b Y, W = a X b Y\), 求 \(U\) 与 \(W\) 的相关系数 \(&rho;_{UW}\). \(E(a X + b Y) = a E(X) + b E(Y)\) \(E(X Y) = E(X) E(Y)\) \(\mathrm{cov}(a X + b Y, Z) = a \mathrm{cov}(X, Z) + b \mathrm{cov}(Y, Z)\) 随机变量的数字特征 期望值 \(E(g(X)) = &int; g(X) f(X) \mathrm{d} X\) Linear Property: \(E(&sum; a_{i} X_{i}) = a_{i} E(X_{i})\) 中位数 (\(p\) 分位数) 最可几值 (\(x |_{P(x) &rarr; \mathrm{maximum}}\)) 矩 \(&alpha;_{l} = E((X - C)^{l})\) 一阶原点矩 \(&mu; = E(X)\) 二阶中心矩 \(&sigma;^{2} = E(X - &mu;)^{2} = V(X)\) Quick Calc: \(V(&sum; a_{i} X_{i}) = &sum; a_{i}^{2} V(X_{i}) + 2 &sum; a_{i} a_{j} \mathrm{cov}(X_{i}, X_{j})\) 偏度 \(&gamma;_{1} = \frac{&mu;_{3}}{&mu;_{2}^{3/2}} = \frac{E((X - &mu;)^{3})}{&sigma;^{3}}\) 随机变量对其均值的不对称程度, 偏斜程度 峰度 \(&gamma;_{2} = \frac{&mu;_{4}}{&mu;_{2}^{2}} - 3 = \frac{E((X - &mu;)^{4})}{&sigma;^{4}} - 3\) 概率密度的尖锐程度于正态分布概率曲线尖锐程度的对比 切比雪夫不等式 \(P(\left| X - &mu; \right| \geq &epsilon;) \leq \frac{&sigma;^{2} }{&epsilon;^{2} }\) GPT: 不关心随机变量服从什么具体的分布, 只要知道期望 (均值) 和方差, 就能给概率划定一个界限. 令 \(&epsilon; = k &sigma;\), 即 \(P(|X - &mu;| \geq k &sigma;) \leq \frac{1}{k^{2}}\), 限定了和均值偏差大于 \(k &sigma;\) 的概率的上界. GPT 感觉课件里面一堆乱七八糟的东西, 有点像是只言片语的梦话连不成线; 或者是有用的信息在长长的文字和定义证明中失去了信息的传递能力. 低情商发言就是: 不如 AI. 以后老师不如就给大纲和要点, 让学生问 AI 算了. 随机变量的特征函数 \(\varphi_{X}(t) = E(e^{i t X})\) GPT: 等价于傅里叶变换为频域. \(&lambda;_{n} = E(X^{n}) = i^{-n} \left[ \frac{\mathrm{d}^{n} \varphi_{X}(t)}{\mathrm{d} t^{n}} \right]\) 随机变量的分布 多项式分布 泊松分布 均匀分布 正态分布 多维随机变量及其分布 二维随机变量的分布, 独立性 \(F(x, y) = P(X \leq x &and; Y \leq y)\) 独立性 Definition: \(f(x, y) = f_{X}(x) f_{Y}(y)\) 条件概率分布 二维随机变量的数字特征 协方差 \(\mathrm{cov}(X, Y) = E((X - &mu;_{X})(Y - &mu;_{Y})) = E(XY) - E(X) E(Y)\) 相关系数 \(&rho;_{XY} = \frac{\mathrm{cov}(X, Y)}{&sigma;_{X} &sigma;_{Y}}\) Memonic: \(\mathrm{cov}(X, X) = &sigma;_{X}^{2}\) 协方差矩阵 \(V_{ij} = \mathrm{cov}(X_{i}, X_{j})\) 两个随机变量的函数的分布 多维随机变量, 向量和矩阵记号 随机变量分布 中心极限定理 GPT: 样本量 \(n\) 足够大, 均值分布趋向于正态分布. Application: 二项分布的正态近似 &#8230; 子样及分布 随机子样, 子样分布函数 子样 子样空间 子样分布函数 统计量及其数字特征 子样中位数 子样平均: \(\bar{x} = \frac{1}{n} &sum; x_{i}\) 子样方差: \(S^{2} = \frac{1}{n-1} &sum; (x_{i} - \bar{x})^{2} = \frac{1}{n - 1} \left( (&sum; x_{i}^{2})- n \bar{x}^{2} \right)\) 子样标准值: \(S\) 子样协方差: \(S_{xy} = \frac{1}{n-1} &sum; (x_{i} - x) (y_{i} - y) = \frac{1}{n-1} \left( &sum; x_{i} y_{i} - n \bar{x} \bar{y} \right)\) 子样相关系数: \(&rho;_{xy} = \frac{S_{XY}}{S_{X} S_{Y}} = \frac{&sum; (x_{i} - \bar{x})(y_{i} - \bar{y})}{(&sum; x_{i}^{2} - n \bar{x}^{2} )^{1/2} (&sum; y_{i}^{2} - n \bar{y}^{2})^{1/2}}\) Memonic: 和随机变量一样, 但是平均的项变为 \(1/(n-1)\). Application 根据子样数据计算数字特征 抽样分布 \(&chi;^{2}\) 分布: 总体服从正态分布时, 样本方差与总体方差的比值服从卡方分布 \(t\) 分布: 样本均值于标准误差的比值服从 t 分布 \(F\) 分布: 两个样本方差的比值 Application 已知子样总体, 计算统计量的概率分布 抽样数据的图形表示, 概率分布 参数估计的一般概念 估计量, 似然函数 区间估计 Algorithm: Application: 单正态总体的均值 单正态总体的方差 双正态总体的均值差 正态总体方差的置信区间 Algorithm Application 已知样本总体, 给定置信水平, 计算置信区间 估计对象已知条件置信区间公式 均值 \(&mu;\)\(&sigma;^{2}\) 已知\(\left[ \bar{x} \mp Z_{&alpha;/2} \frac{&sigma;}{\sqrt{n}} \right]\) 均值 \(&mu;\)\(&sigma;^{2}\) 未知\(\left[ \bar{x} \mp t_{&alpha;/2}(n-1) \frac{S}{\sqrt{n}} \right]\) 方差 \(&sigma;^{2}\)\(&mu;\) 未知\(\left[ \frac{(n-1) S^2}{&chi;_{&alpha;/2}^{2}(n-1)}, \frac{(n-1) S^{2}}{&chi;_{1-&alpha;/2}^{2} (n-1)} \right]\) Calculation: \(Z_{&alpha;/2} = &Phi;^{-1}(1 - &alpha;/2)\), where \(&Phi;\) 为 \(N(0, 1)\) 正态分布 \(t_{&alpha;/2}(n)\) 为自由度为 \(n-1\) 的 Student 分布 \(&chi;_{&alpha;/2}^{2}(n)\), where \(&chi;^{2}(n)\) 为自由度为 \(n-1\) 的卡方分布 明明是查表才能知道的东西&#8230; 极大似然法 极大似然原理 Algorithm: 构造函数: \(L(\left\{ x_{i} \right\} | &theta;) = &prod; f(x_{i}|&theta;)\) 极大似然条件约束下求解 \(&theta;\): \(&part;_{&theta;} ln L = 0 &and; &part;_{&theta;}^{2} ln L &lt; 0\) Application: 参数区间估计 (似然区间) Algorithm: \(&gamma; = \frac{&int;_{&theta;_{a}}^{&theta;_{b}} L(X|&theta;) \mathrm{d}&theta;}{&int;_{-&infin;}^{&infin;} L(X|&theta;) \mathrm{d}&theta;}\) 同上, 在 \(&part;_{&theta;} &gamma; = 0 &and; &part;_{&theta;}^{2} &gamma; &lt; 0\) 条件下求解 子样观测值 几何分布 设 \(x_{1}, \cdots, x_{n}\) 是几何分布总体的子样观测值, 其分布律为 \(P(X = x) = p (1 - p)^{x-1}, x = 1, 2, \cdots, &infin;\). 求参数 \(p\) 的极大似然估计. \(L(\left\{ x_{i} \right\} | p) = &prod; p(1-p)^{x-1}\) \(&part;_{p} ln L = &part;_{p} ln \left( p^{n} (1-p)^{&sum; x_{i} - n} \right) = \frac{n - p &sum; x_{i}}{p(1-p)}\) 极大似然法应用于多个实验结果的合并 \(&mu; = \frac{&sum; \frac{&mu;_{i}}{&sigma;_{i}^{2}}}{ &sum; \frac{1}{ &sigma;_{i}^{2 }} } \) \(V = \frac{1}{&sum; \frac{1}{&sigma;_{i}^{2} }}\) 极大似然法用于直方图 最小二乘法 最小二乘拟合 Algorithm: 构造误差函数 \(\mathrm{err}(&theta;) = &sum; (y_{i} - f(x_{i} | &theta;))^{2}\) 最小化误差 \(&part;_{&theta;} \mathrm{err} = 0 &and; &part;_{&theta;}^{2} \mathrm{err} &gt; 0\) 最小二乘用于直方图数据 假设检验 原假设和备择假设 参数检验: 根据观测值检验参数是否等于某个给定值 非参数检验: 根据观测值检验模型函数是否有某个特定函数形式 原假设: 要验证的假设为原假设 备择假设 假设检验的一般方法 Algorithm 原假设和备择假设 (Input) 原假设成立时的统计量 计算统计量观测值 计算拒绝域 判断统计决策 Application 已知正态分布 (子样分布) 和方差 某厂生产的一种电池, 其寿命长期以来服从方差 \(&sigma;^{2} = 5000\ \mathrm{h}^{2}\) 的正态分布. 近期生产一批这种电池, 从生产的情况来看不能肯定寿命方差是否改变. 现随机地取 26 个电池, 测得寿命的样本方差为 \(S^{2} = 9200\ \mathrm{h}^{2}\). 问根据这一数据能否推断这批电池寿命方差较以往有显著变换 (取 \(&alpha; = 0.02\))? 正态总体的参数检验 拟合优度检验 \(&chi;^{2}\) 检验 Definition: \(&chi;^{2} = &sum; \frac{(x_{i} - X)^{2}}{X}\) 似然比检验 信号的统计显著性 贝叶斯公式 Monte Carlo 方法 实际使用过程中, Monte Carlo 方法需要很好地构造随机过程&#8230; 模式识别系统 乐, 只有大段文字, 仿佛在看本科生大学写作课程作业的综述论文. (而且都还很老, 并且不知道为啥, 物理系这边的 AI 课特别喜欢讲 Hopfield 网络, 哪怕他们讲的 Hopfield 网络都不太能用, 或者说不知道该怎么用. ) 理论课的通病 不知道为啥, 讲理论课的老师往往会把定理/算法/规律的严格证明视作是实现的正确性, 好像是只要我证明了我的做法是对的, 那么做的结果就一定是对的. 然后对于如何评价实际结果的正确性的方法几乎不提. 搞清楚课程定位啊淦, 这课不是 物理 中的概率统计吗? 不是实验类的课程吗? 别告诉我说你们实验组有祖传代码拟合分析误差自动生成论文配图, 只需要知道自己的原理是对的是吧&#8230; (当然, 假设检验确实是一个可用的方法, 但是什么叫做假设检验课在证明假设检验是对的? ) 又, 还有的困难是引入的新理论和旧的理论接不上, 有种粘连的感觉. 这有种讲分析力学讲了半天的拉格朗日方程, 结果最后发现用的是牛顿三定律一样的奇怪. 应该就直接抛弃旧的知识, 用新的视角去讲整个 (要么就干脆承认, 说我们不讲新的). 在本科赵爹的课上的那个做法就很不错, 用新的视角去引入 (上来就是对称性守恒律, 相信基础教育的牛顿三定律的普及教育, 而不是重新教一遍牛顿三定律再开始讲能量视角), 然后在打好了新的视角的基础之后, 再回过头去审视旧的理论如何迁移到新的视角里. 比如在概统的这边, 我觉得确实可以用测度空间的方法来解释很多东西, 然后再回过头去讲其他东西, 就会很不错. 不过可能也要看具体的应用或者教学需求, 用测度论来讲实验误差分析的话, 感觉不是很好理解 (也有可能只是我不会, 之后可以重新学), 但是如果直接从实验误差为主线来重新讲课, 那估计就能够砍掉很多的垃圾学时, 并且还能更有条理&#8230; 特征提取和选择 效率, 误判率, 分辨能力 效率: 正确选定一个信号事例的效率 \(&epsilon;_{SS} = &int; f_{S}(y) \mathrm{d}y\) 误判率: 将本底错误选择为一个信号事例的误判率 \(&epsilon;_{SB} = &int; f_{B}(y) \mathrm{d}y\) 误判率: 效率与误判率的比 \(r = &epsilon;_{SS} / &epsilon;_{SB}\) Memonic: 引入误差矩阵 判断 \ 实际真假 真真真真假 假假真假假 效率: 真真 / (真真 + 假真) 误判率: 真假 / (真假 + 假假) 方差: \(V(&epsilon;_{SS}) = \frac{&epsilon;_{SS} (1 - &epsilon;_{SS})}{N_{S}}\) \(V(r)/r^{2} = V(&epsilon;_{SS}) / &epsilon;_{SS}^{2} + V(&epsilon;_{SB}) / &epsilon;_{SB}^{2}\) 方法 贝叶斯决策 线性判别 决策树方法 人工神经网络 近邻法 概率密度估计量方法 H 矩阵判别 函数判别 SVM 小信号测量的区间估计 测量误差及其分类 和子样误差放在一起 课上把测量误差和子样误差分开来讲了, 实际上我觉得这两个东西应当放在一起讲, 毕竟这两个才是真的相关的知识, 也是在实验里真的用的东西. 明明可以用一个实验数据的采集的全流程来讲随机子样, 子样分布函数这些知识, 非要用概念朗诵, 鼠标扫扫公式 &#8211; 大家要记住哦, 很重要哦, 这种方式来教课&#8230; 不过也可以理解就是了 &#8211; who cares? Application: 误差传递公式 探测器效率计算: 实际计数 / 源计数 Application: 多层符合探测效率 m 层中至少 k 触发: \(&epsilon;_{\mathrm{tol}} = &sum; \left(\begin{matrix}m &#92;&#92; k\end{matrix}\right) &epsilon;^{i} (1 - &epsilon;)^{m-i}\)" />
<meta property="og:description" content="About 挺一般的研究生课程 评价是纯纯的 PPT 朗诵. Memonic 贝叶斯公式 \(P(B_{i} | A) = \frac{P(A|B_{i})P(B_{i})}{&sum;_{j=1}^{n} P(A|B_{j}) P(B_{j})}\) 随机变量映射 \(F_{Y}(Y) = P(g(X) \leq Y)\) 统计量数字特征计算 特征随机变量子样 平均\(\frac{1}{n} &sum; x_i\)\(\frac{1}{n} &sum; x_{i}\) 方差\(&sigma;^{2} = &sum; (X - \bar{X})^{2} = &sum; X^{2} - (&sum; X)^{2}\)\(S^{2} = \frac{1}{n-1} &sum; (x_{i} - \bar{x}_{i})^{2} = \frac{1}{n-1} ((&sum; x_{i}^{2}) - n \bar{x}^{2})\) 多元方差\(V(&sum; a_{i} X_{i}) = &sum; a_{i}^{2} V(X_{i}) + &sum; a_{i} a_{j} \mathrm{cov}(X_{i}, X_{j})\)- 协方差\(\mathrm{cov}(X, Y) = E((X - E(X))(Y - E(Y))) = \mathrm{cov}(X, Y) = E(XY) - E(X)E(Y)\)\(S_{XY} = \frac{1}{n-1} &sum; (x_{i} - \bar{x})(y_{i} - \bar{y})\) 相关系数\(&rho;_{XY} = \mathrm{cov}(X, Y) / (&sigma;_{X} &sigma;_{Y})\)\(r = \frac{&sum; (x_{i} - \bar{x})(y_{i} - \bar{y})}{\sqrt{(x_{i} - \bar{x})} \sqrt{(y_{i} - \bar{y})}}\) 实验数据合并 \[&mu; = \frac{&sum; \frac{&mu;_{i}}{&sigma;_{i}^{2}}}{ &sum; \frac{1}{ &sigma;_{i}^{2 }} },\ V = \frac{1}{&sum; \frac{1}{&sigma;_{i}^{2} }}\] 极大似然法 \[L = &prod; f(x_{i}|&theta;),\ &part;_{&theta;} ln L = 0 &and; &part;_{&theta;}^{2} ln L &lt; 0\] 最小二乘法 \[\mathrm{err}(&theta;) = &sum; (y_{i} - f(x_{i}|&theta;))^{2},\ &part;_{&theta;} \mathrm{err} = 0 &and; &part;_{&theta;}^{2} \mathrm{err} &gt; 0\] 假设检验 条件检验对象统计量观测值拒绝域 \(&sigma;^{2}\) 已知均值 \(&mu;\)\(Z = \frac{\bar{X} - &mu;_{0}}{&sigma; / \sqrt{n}}\)\(\left\vert Z \right\vert &gt; Z_{&alpha;/2}(n-1)\) \(&sigma;^{2}\) 未知均质 \(&mu;\)\(t = \frac{\bar{X} - &mu;_{0}}{S / \sqrt{n}}\)\(\left\vert t \right\vert &gt; t_{&alpha;/2}(n-1)\) 方差 \(&sigma;^{2}\)\(&chi;^{2} = \frac{(n-1) S^{2}}{&sigma;_{0}^{2}}\)\(&chi;^{2} &lt; &chi;_{1-&alpha;/2}^{2}(n-1) &or; &chi;^{2} &gt; &chi;_{&alpha;/2}^{2}(n-1)\) 几个分布 分布PDFE(X)V(X)Notes 二项分布 \(B(n, p)\)\(\left(\begin{matrix} n &#92;&#92; k\end{matrix}\right) p^{k} (1-p)^{n-k}\)\(np\)\(np(1-p)\)(多项式分布) 泊松分布 \(P(&lambda;)\)\(\frac{&lambda;^{k} \mathrm{e}^{-&lambda;}}{k!}\)\(&lambda;\)\(&lambda;\)粒子计数标准差 \(\sqrt{N}\) 正态分布 \(N(&mu;,&sigma;^{2})\)\(\frac{1}{\sqrt{2 &pi;} &sigma;} \mathrm{exp}\left( - \frac{(x-&mu;)^{2}}{2&sigma;^{2}} \right)\)\(&mu;\)\(&sigma;\) 概率论初步 随机试验, 随机事件, 样本空间 GPT: 用一句话解释名词概念 必然现象: 在特定条件下必定发生的现象, 任何情况下都会发生 随机现象: 其结果不可预知, 受随机因素影响的现象 随机试验: 可以重复进行且结果不确定的实验或过程 互相独立: 两个或多个事件的发生与否互不影响 随机事件: 随机试验中可能发生的结果或现象 基本事件: 在随机试验中不再可以分解的最基本的事件 样本空间: 随机试验中所有可能基本事件的集合 元素: 样本空间中的每一个基本事件 必然事件: 一定会发生的事件, 其概率为 1 不可能事件: 一定不会发生的事件, 其概率为 0 事件关系 事件运算 \(A &and; B\), \(A &or; B\), \(&not; A\) 概率 Application 多项式分布 盒子里有 5 个红球, 4 个白球和 3 个蓝球, 从盒子里随机选一球, 记下颜色, 然后返回盒子里. 求挑选 6 次后出现 3 红, 2 白, 1 蓝的概率. 有放回取样 (多项式分布): \[P(\left\{ p_{i}, n_{i} \right\}) = \frac{(&sum; n_{i})!}{&prod; n_{i}!} &prod; p_{i}^{n_{i}}\] 无放回取样 (超几何分布): \[P(\left\{ p_{i}, n_{i} \right\}) = \frac{&prod; \left(\begin{matrix} N_{i} &#92;&#92; n_{i} \end{matrix}\right)}{\left(\begin{matrix} &sum; N_{i} &#92;&#92; &sum; n_{i} \end{matrix}\right)}\] Definition: \(\left(\begin{matrix} N &#92;&#92; n \end{matrix}\right) = \frac{N &times; (N - 1) &times; \cdots &times; (n+1)}{n &times; \cdots &times; 1}\) 条件概率 \(P(B|A) = \frac{P(A &and; B)}{P(A)}\) 事件独立性 Definition: \(P(A B) = P(A) P(B) &hArr; P(B|A) = P(B) &hArr;\) A, B 独立 Application 判断随机变量的独立性 设随机变量 \(X, Y\) 的 PDF 为 \(f(x, y) = A \mathrm{e}^{- a x^2 + b x y - c y^2}, - &infin; &lt; x &lt; &infin;, - &infin; &lt; y &lt; &infin;\), 问在什么条件下 \(X\) 与 \(Y\) 相互独立? 证明需要证明 \(f(x, y) = f_{X}(x) f_{Y}(y)\) 概率计算 边沿概率, 全概率公式, 贝叶斯公式 边沿概率 Application: 已知联合分布, 计算边缘分布 补全联合分布 全概率公式 \(P(A) = &sum; P(A | B_{i}) P(B_{i})\), 其中 \(\left\{ B_{i} \right\}\) 为样本空间的划分 贝叶斯公式 \(P(B_{i} | A) = \frac{P(A|B_{i})P(B_{i})}{&sum;_{j=1}^{n} P(A|B_{j}) P(B_{j})}\) Memonic: \(P(B_{i} | A) P(A) = P(B_{i} &and; A) = P(A | B_{i}) P(B_{i})\) Application: 误判概率计算 (逆概率计算) 已知某放射源产生的射线中包含 A, B, C 三种粒子, 占比分别 1/2, 1/6, 1/3. 在实验的粒子鉴别过程中, A 粒子被误判为其他粒子的概率为 10%, B, C 粒子被误判为 A 粒子的概率分别为 12% 和 21%. 请计算出被鉴别为 A 粒子的事例来自真实的 A 粒子的概率. 已知 \(P(S_{A})\) (\(S_{*}\) 对样本空间的划分) 以及 \(P(D_{A} | S_{ *})\) 求 \(P(S_{A} | D_{A}) = \frac{P(D_{A} | S_{A}) P(S_{A})}{&sum; P(D_{A}|S_{})P(S_{})\) 随机变量及其分布 随机变量 为了和之后的子样 (\(x_{i}\)) 作区分, 这里用 \(X\) 表示随机变量 数学符号一坨乱麻 不知道是不是 PPT 里面的数学公式和文本是东拼西凑的, 里面的数学公式符号非常的不一致&#8230; 学起来很混乱. 概率密度函数 (PDF): \(f(X)\) 累计分布函数 (CDF): \(F(X) = &int; f(X) \mathrm{d}x\) 随机变量的映射: Algorithm I: \(F_{y}(y) = P(g(x) \leq y)\) Algorithm II: \(g\) 为映射作用在随机变量 \(X\) 上, 则 \(f_{Y}(Y) = f_{X}(g^{-1}(Y)) \left| \frac{\mathrm{d}}{\mathrm{d} Y} g^{-1}(Y) \right|\). Memonic: \(F_{Y}(Y) = &int; f_{Y}(Y) \mathrm{d} Y = &int; f_{X}(X) \mathrm{d} X \frac{\mathrm{d} X}{\mathrm{d} Y} &rArr; f_{Y} = f_{X}(g^{-1}(Y)) \frac{\mathrm{d} X}{\mathrm{d} Y}\) Application: 计算概率密度函数映射 随机变量 \(X\) 的概率密度函数为: \[f(x) = \left\{ \begin{matrix} x^2 / 18 &amp; -3 &lt; x &lt; 3 &#92;&#92; 0 &amp; \mathrm{other} \end{matrix} \right.\] 求 \(Y = (X + 1)^2\) 的概率密度函数. 非单调映射 \(F_{Y}(y) = P(g(X) \leq y)\) 计算变换关系 已知 \(X, Y\) 为 \((0, 1)\) 区间上均匀分布的随机变量且相互独立, 二维随机变量 \(U, V\) 和 \(X, Y\) 之间存在变换关系: \(U = cos (2 &pi; x) \sqrt{- 2 ln y}, V = sin (2 &pi; x) \sqrt{- 2 ln y}\), 证明 \(U, V\) 服从 \(N(0, 1)\). 映射的数字特征 随机变量 \(X, Y\) 都服从 \(N(&mu;, &sigma;^2)\) 且相互独立, 令 \(U = a X + b Y, W = a X b Y\), 求 \(U\) 与 \(W\) 的相关系数 \(&rho;_{UW}\). \(E(a X + b Y) = a E(X) + b E(Y)\) \(E(X Y) = E(X) E(Y)\) \(\mathrm{cov}(a X + b Y, Z) = a \mathrm{cov}(X, Z) + b \mathrm{cov}(Y, Z)\) 随机变量的数字特征 期望值 \(E(g(X)) = &int; g(X) f(X) \mathrm{d} X\) Linear Property: \(E(&sum; a_{i} X_{i}) = a_{i} E(X_{i})\) 中位数 (\(p\) 分位数) 最可几值 (\(x |_{P(x) &rarr; \mathrm{maximum}}\)) 矩 \(&alpha;_{l} = E((X - C)^{l})\) 一阶原点矩 \(&mu; = E(X)\) 二阶中心矩 \(&sigma;^{2} = E(X - &mu;)^{2} = V(X)\) Quick Calc: \(V(&sum; a_{i} X_{i}) = &sum; a_{i}^{2} V(X_{i}) + 2 &sum; a_{i} a_{j} \mathrm{cov}(X_{i}, X_{j})\) 偏度 \(&gamma;_{1} = \frac{&mu;_{3}}{&mu;_{2}^{3/2}} = \frac{E((X - &mu;)^{3})}{&sigma;^{3}}\) 随机变量对其均值的不对称程度, 偏斜程度 峰度 \(&gamma;_{2} = \frac{&mu;_{4}}{&mu;_{2}^{2}} - 3 = \frac{E((X - &mu;)^{4})}{&sigma;^{4}} - 3\) 概率密度的尖锐程度于正态分布概率曲线尖锐程度的对比 切比雪夫不等式 \(P(\left| X - &mu; \right| \geq &epsilon;) \leq \frac{&sigma;^{2} }{&epsilon;^{2} }\) GPT: 不关心随机变量服从什么具体的分布, 只要知道期望 (均值) 和方差, 就能给概率划定一个界限. 令 \(&epsilon; = k &sigma;\), 即 \(P(|X - &mu;| \geq k &sigma;) \leq \frac{1}{k^{2}}\), 限定了和均值偏差大于 \(k &sigma;\) 的概率的上界. GPT 感觉课件里面一堆乱七八糟的东西, 有点像是只言片语的梦话连不成线; 或者是有用的信息在长长的文字和定义证明中失去了信息的传递能力. 低情商发言就是: 不如 AI. 以后老师不如就给大纲和要点, 让学生问 AI 算了. 随机变量的特征函数 \(\varphi_{X}(t) = E(e^{i t X})\) GPT: 等价于傅里叶变换为频域. \(&lambda;_{n} = E(X^{n}) = i^{-n} \left[ \frac{\mathrm{d}^{n} \varphi_{X}(t)}{\mathrm{d} t^{n}} \right]\) 随机变量的分布 多项式分布 泊松分布 均匀分布 正态分布 多维随机变量及其分布 二维随机变量的分布, 独立性 \(F(x, y) = P(X \leq x &and; Y \leq y)\) 独立性 Definition: \(f(x, y) = f_{X}(x) f_{Y}(y)\) 条件概率分布 二维随机变量的数字特征 协方差 \(\mathrm{cov}(X, Y) = E((X - &mu;_{X})(Y - &mu;_{Y})) = E(XY) - E(X) E(Y)\) 相关系数 \(&rho;_{XY} = \frac{\mathrm{cov}(X, Y)}{&sigma;_{X} &sigma;_{Y}}\) Memonic: \(\mathrm{cov}(X, X) = &sigma;_{X}^{2}\) 协方差矩阵 \(V_{ij} = \mathrm{cov}(X_{i}, X_{j})\) 两个随机变量的函数的分布 多维随机变量, 向量和矩阵记号 随机变量分布 中心极限定理 GPT: 样本量 \(n\) 足够大, 均值分布趋向于正态分布. Application: 二项分布的正态近似 &#8230; 子样及分布 随机子样, 子样分布函数 子样 子样空间 子样分布函数 统计量及其数字特征 子样中位数 子样平均: \(\bar{x} = \frac{1}{n} &sum; x_{i}\) 子样方差: \(S^{2} = \frac{1}{n-1} &sum; (x_{i} - \bar{x})^{2} = \frac{1}{n - 1} \left( (&sum; x_{i}^{2})- n \bar{x}^{2} \right)\) 子样标准值: \(S\) 子样协方差: \(S_{xy} = \frac{1}{n-1} &sum; (x_{i} - x) (y_{i} - y) = \frac{1}{n-1} \left( &sum; x_{i} y_{i} - n \bar{x} \bar{y} \right)\) 子样相关系数: \(&rho;_{xy} = \frac{S_{XY}}{S_{X} S_{Y}} = \frac{&sum; (x_{i} - \bar{x})(y_{i} - \bar{y})}{(&sum; x_{i}^{2} - n \bar{x}^{2} )^{1/2} (&sum; y_{i}^{2} - n \bar{y}^{2})^{1/2}}\) Memonic: 和随机变量一样, 但是平均的项变为 \(1/(n-1)\). Application 根据子样数据计算数字特征 抽样分布 \(&chi;^{2}\) 分布: 总体服从正态分布时, 样本方差与总体方差的比值服从卡方分布 \(t\) 分布: 样本均值于标准误差的比值服从 t 分布 \(F\) 分布: 两个样本方差的比值 Application 已知子样总体, 计算统计量的概率分布 抽样数据的图形表示, 概率分布 参数估计的一般概念 估计量, 似然函数 区间估计 Algorithm: Application: 单正态总体的均值 单正态总体的方差 双正态总体的均值差 正态总体方差的置信区间 Algorithm Application 已知样本总体, 给定置信水平, 计算置信区间 估计对象已知条件置信区间公式 均值 \(&mu;\)\(&sigma;^{2}\) 已知\(\left[ \bar{x} \mp Z_{&alpha;/2} \frac{&sigma;}{\sqrt{n}} \right]\) 均值 \(&mu;\)\(&sigma;^{2}\) 未知\(\left[ \bar{x} \mp t_{&alpha;/2}(n-1) \frac{S}{\sqrt{n}} \right]\) 方差 \(&sigma;^{2}\)\(&mu;\) 未知\(\left[ \frac{(n-1) S^2}{&chi;_{&alpha;/2}^{2}(n-1)}, \frac{(n-1) S^{2}}{&chi;_{1-&alpha;/2}^{2} (n-1)} \right]\) Calculation: \(Z_{&alpha;/2} = &Phi;^{-1}(1 - &alpha;/2)\), where \(&Phi;\) 为 \(N(0, 1)\) 正态分布 \(t_{&alpha;/2}(n)\) 为自由度为 \(n-1\) 的 Student 分布 \(&chi;_{&alpha;/2}^{2}(n)\), where \(&chi;^{2}(n)\) 为自由度为 \(n-1\) 的卡方分布 明明是查表才能知道的东西&#8230; 极大似然法 极大似然原理 Algorithm: 构造函数: \(L(\left\{ x_{i} \right\} | &theta;) = &prod; f(x_{i}|&theta;)\) 极大似然条件约束下求解 \(&theta;\): \(&part;_{&theta;} ln L = 0 &and; &part;_{&theta;}^{2} ln L &lt; 0\) Application: 参数区间估计 (似然区间) Algorithm: \(&gamma; = \frac{&int;_{&theta;_{a}}^{&theta;_{b}} L(X|&theta;) \mathrm{d}&theta;}{&int;_{-&infin;}^{&infin;} L(X|&theta;) \mathrm{d}&theta;}\) 同上, 在 \(&part;_{&theta;} &gamma; = 0 &and; &part;_{&theta;}^{2} &gamma; &lt; 0\) 条件下求解 子样观测值 几何分布 设 \(x_{1}, \cdots, x_{n}\) 是几何分布总体的子样观测值, 其分布律为 \(P(X = x) = p (1 - p)^{x-1}, x = 1, 2, \cdots, &infin;\). 求参数 \(p\) 的极大似然估计. \(L(\left\{ x_{i} \right\} | p) = &prod; p(1-p)^{x-1}\) \(&part;_{p} ln L = &part;_{p} ln \left( p^{n} (1-p)^{&sum; x_{i} - n} \right) = \frac{n - p &sum; x_{i}}{p(1-p)}\) 极大似然法应用于多个实验结果的合并 \(&mu; = \frac{&sum; \frac{&mu;_{i}}{&sigma;_{i}^{2}}}{ &sum; \frac{1}{ &sigma;_{i}^{2 }} } \) \(V = \frac{1}{&sum; \frac{1}{&sigma;_{i}^{2} }}\) 极大似然法用于直方图 最小二乘法 最小二乘拟合 Algorithm: 构造误差函数 \(\mathrm{err}(&theta;) = &sum; (y_{i} - f(x_{i} | &theta;))^{2}\) 最小化误差 \(&part;_{&theta;} \mathrm{err} = 0 &and; &part;_{&theta;}^{2} \mathrm{err} &gt; 0\) 最小二乘用于直方图数据 假设检验 原假设和备择假设 参数检验: 根据观测值检验参数是否等于某个给定值 非参数检验: 根据观测值检验模型函数是否有某个特定函数形式 原假设: 要验证的假设为原假设 备择假设 假设检验的一般方法 Algorithm 原假设和备择假设 (Input) 原假设成立时的统计量 计算统计量观测值 计算拒绝域 判断统计决策 Application 已知正态分布 (子样分布) 和方差 某厂生产的一种电池, 其寿命长期以来服从方差 \(&sigma;^{2} = 5000\ \mathrm{h}^{2}\) 的正态分布. 近期生产一批这种电池, 从生产的情况来看不能肯定寿命方差是否改变. 现随机地取 26 个电池, 测得寿命的样本方差为 \(S^{2} = 9200\ \mathrm{h}^{2}\). 问根据这一数据能否推断这批电池寿命方差较以往有显著变换 (取 \(&alpha; = 0.02\))? 正态总体的参数检验 拟合优度检验 \(&chi;^{2}\) 检验 Definition: \(&chi;^{2} = &sum; \frac{(x_{i} - X)^{2}}{X}\) 似然比检验 信号的统计显著性 贝叶斯公式 Monte Carlo 方法 实际使用过程中, Monte Carlo 方法需要很好地构造随机过程&#8230; 模式识别系统 乐, 只有大段文字, 仿佛在看本科生大学写作课程作业的综述论文. (而且都还很老, 并且不知道为啥, 物理系这边的 AI 课特别喜欢讲 Hopfield 网络, 哪怕他们讲的 Hopfield 网络都不太能用, 或者说不知道该怎么用. ) 理论课的通病 不知道为啥, 讲理论课的老师往往会把定理/算法/规律的严格证明视作是实现的正确性, 好像是只要我证明了我的做法是对的, 那么做的结果就一定是对的. 然后对于如何评价实际结果的正确性的方法几乎不提. 搞清楚课程定位啊淦, 这课不是 物理 中的概率统计吗? 不是实验类的课程吗? 别告诉我说你们实验组有祖传代码拟合分析误差自动生成论文配图, 只需要知道自己的原理是对的是吧&#8230; (当然, 假设检验确实是一个可用的方法, 但是什么叫做假设检验课在证明假设检验是对的? ) 又, 还有的困难是引入的新理论和旧的理论接不上, 有种粘连的感觉. 这有种讲分析力学讲了半天的拉格朗日方程, 结果最后发现用的是牛顿三定律一样的奇怪. 应该就直接抛弃旧的知识, 用新的视角去讲整个 (要么就干脆承认, 说我们不讲新的). 在本科赵爹的课上的那个做法就很不错, 用新的视角去引入 (上来就是对称性守恒律, 相信基础教育的牛顿三定律的普及教育, 而不是重新教一遍牛顿三定律再开始讲能量视角), 然后在打好了新的视角的基础之后, 再回过头去审视旧的理论如何迁移到新的视角里. 比如在概统的这边, 我觉得确实可以用测度空间的方法来解释很多东西, 然后再回过头去讲其他东西, 就会很不错. 不过可能也要看具体的应用或者教学需求, 用测度论来讲实验误差分析的话, 感觉不是很好理解 (也有可能只是我不会, 之后可以重新学), 但是如果直接从实验误差为主线来重新讲课, 那估计就能够砍掉很多的垃圾学时, 并且还能更有条理&#8230; 特征提取和选择 效率, 误判率, 分辨能力 效率: 正确选定一个信号事例的效率 \(&epsilon;_{SS} = &int; f_{S}(y) \mathrm{d}y\) 误判率: 将本底错误选择为一个信号事例的误判率 \(&epsilon;_{SB} = &int; f_{B}(y) \mathrm{d}y\) 误判率: 效率与误判率的比 \(r = &epsilon;_{SS} / &epsilon;_{SB}\) Memonic: 引入误差矩阵 判断 \ 实际真假 真真真真假 假假真假假 效率: 真真 / (真真 + 假真) 误判率: 真假 / (真假 + 假假) 方差: \(V(&epsilon;_{SS}) = \frac{&epsilon;_{SS} (1 - &epsilon;_{SS})}{N_{S}}\) \(V(r)/r^{2} = V(&epsilon;_{SS}) / &epsilon;_{SS}^{2} + V(&epsilon;_{SB}) / &epsilon;_{SB}^{2}\) 方法 贝叶斯决策 线性判别 决策树方法 人工神经网络 近邻法 概率密度估计量方法 H 矩阵判别 函数判别 SVM 小信号测量的区间估计 测量误差及其分类 和子样误差放在一起 课上把测量误差和子样误差分开来讲了, 实际上我觉得这两个东西应当放在一起讲, 毕竟这两个才是真的相关的知识, 也是在实验里真的用的东西. 明明可以用一个实验数据的采集的全流程来讲随机子样, 子样分布函数这些知识, 非要用概念朗诵, 鼠标扫扫公式 &#8211; 大家要记住哦, 很重要哦, 这种方式来教课&#8230; 不过也可以理解就是了 &#8211; who cares? Application: 误差传递公式 探测器效率计算: 实际计数 / 源计数 Application: 多层符合探测效率 m 层中至少 k 触发: \(&epsilon;_{\mathrm{tol}} = &sum; \left(\begin{matrix}m &#92;&#92; k\end{matrix}\right) &epsilon;^{i} (1 - &epsilon;)^{m-i}\)" />
<link rel="canonical" href="/notes/stastics/" />
<meta property="og:url" content="/notes/stastics/" />
<meta property="og:site_name" content="My Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2026-01-19T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="概统" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2026-01-19T00:00:00+00:00","datePublished":"2026-01-19T00:00:00+00:00","description":"About 挺一般的研究生课程 评价是纯纯的 PPT 朗诵. Memonic 贝叶斯公式 \\(P(B_{i} | A) = \\frac{P(A|B_{i})P(B_{i})}{&sum;_{j=1}^{n} P(A|B_{j}) P(B_{j})}\\) 随机变量映射 \\(F_{Y}(Y) = P(g(X) \\leq Y)\\) 统计量数字特征计算 特征随机变量子样 平均\\(\\frac{1}{n} &sum; x_i\\)\\(\\frac{1}{n} &sum; x_{i}\\) 方差\\(&sigma;^{2} = &sum; (X - \\bar{X})^{2} = &sum; X^{2} - (&sum; X)^{2}\\)\\(S^{2} = \\frac{1}{n-1} &sum; (x_{i} - \\bar{x}_{i})^{2} = \\frac{1}{n-1} ((&sum; x_{i}^{2}) - n \\bar{x}^{2})\\) 多元方差\\(V(&sum; a_{i} X_{i}) = &sum; a_{i}^{2} V(X_{i}) + &sum; a_{i} a_{j} \\mathrm{cov}(X_{i}, X_{j})\\)- 协方差\\(\\mathrm{cov}(X, Y) = E((X - E(X))(Y - E(Y))) = \\mathrm{cov}(X, Y) = E(XY) - E(X)E(Y)\\)\\(S_{XY} = \\frac{1}{n-1} &sum; (x_{i} - \\bar{x})(y_{i} - \\bar{y})\\) 相关系数\\(&rho;_{XY} = \\mathrm{cov}(X, Y) / (&sigma;_{X} &sigma;_{Y})\\)\\(r = \\frac{&sum; (x_{i} - \\bar{x})(y_{i} - \\bar{y})}{\\sqrt{(x_{i} - \\bar{x})} \\sqrt{(y_{i} - \\bar{y})}}\\) 实验数据合并 \\[&mu; = \\frac{&sum; \\frac{&mu;_{i}}{&sigma;_{i}^{2}}}{ &sum; \\frac{1}{ &sigma;_{i}^{2 }} },\\ V = \\frac{1}{&sum; \\frac{1}{&sigma;_{i}^{2} }}\\] 极大似然法 \\[L = &prod; f(x_{i}|&theta;),\\ &part;_{&theta;} ln L = 0 &and; &part;_{&theta;}^{2} ln L &lt; 0\\] 最小二乘法 \\[\\mathrm{err}(&theta;) = &sum; (y_{i} - f(x_{i}|&theta;))^{2},\\ &part;_{&theta;} \\mathrm{err} = 0 &and; &part;_{&theta;}^{2} \\mathrm{err} &gt; 0\\] 假设检验 条件检验对象统计量观测值拒绝域 \\(&sigma;^{2}\\) 已知均值 \\(&mu;\\)\\(Z = \\frac{\\bar{X} - &mu;_{0}}{&sigma; / \\sqrt{n}}\\)\\(\\left\\vert Z \\right\\vert &gt; Z_{&alpha;/2}(n-1)\\) \\(&sigma;^{2}\\) 未知均质 \\(&mu;\\)\\(t = \\frac{\\bar{X} - &mu;_{0}}{S / \\sqrt{n}}\\)\\(\\left\\vert t \\right\\vert &gt; t_{&alpha;/2}(n-1)\\) 方差 \\(&sigma;^{2}\\)\\(&chi;^{2} = \\frac{(n-1) S^{2}}{&sigma;_{0}^{2}}\\)\\(&chi;^{2} &lt; &chi;_{1-&alpha;/2}^{2}(n-1) &or; &chi;^{2} &gt; &chi;_{&alpha;/2}^{2}(n-1)\\) 几个分布 分布PDFE(X)V(X)Notes 二项分布 \\(B(n, p)\\)\\(\\left(\\begin{matrix} n &#92;&#92; k\\end{matrix}\\right) p^{k} (1-p)^{n-k}\\)\\(np\\)\\(np(1-p)\\)(多项式分布) 泊松分布 \\(P(&lambda;)\\)\\(\\frac{&lambda;^{k} \\mathrm{e}^{-&lambda;}}{k!}\\)\\(&lambda;\\)\\(&lambda;\\)粒子计数标准差 \\(\\sqrt{N}\\) 正态分布 \\(N(&mu;,&sigma;^{2})\\)\\(\\frac{1}{\\sqrt{2 &pi;} &sigma;} \\mathrm{exp}\\left( - \\frac{(x-&mu;)^{2}}{2&sigma;^{2}} \\right)\\)\\(&mu;\\)\\(&sigma;\\) 概率论初步 随机试验, 随机事件, 样本空间 GPT: 用一句话解释名词概念 必然现象: 在特定条件下必定发生的现象, 任何情况下都会发生 随机现象: 其结果不可预知, 受随机因素影响的现象 随机试验: 可以重复进行且结果不确定的实验或过程 互相独立: 两个或多个事件的发生与否互不影响 随机事件: 随机试验中可能发生的结果或现象 基本事件: 在随机试验中不再可以分解的最基本的事件 样本空间: 随机试验中所有可能基本事件的集合 元素: 样本空间中的每一个基本事件 必然事件: 一定会发生的事件, 其概率为 1 不可能事件: 一定不会发生的事件, 其概率为 0 事件关系 事件运算 \\(A &and; B\\), \\(A &or; B\\), \\(&not; A\\) 概率 Application 多项式分布 盒子里有 5 个红球, 4 个白球和 3 个蓝球, 从盒子里随机选一球, 记下颜色, 然后返回盒子里. 求挑选 6 次后出现 3 红, 2 白, 1 蓝的概率. 有放回取样 (多项式分布): \\[P(\\left\\{ p_{i}, n_{i} \\right\\}) = \\frac{(&sum; n_{i})!}{&prod; n_{i}!} &prod; p_{i}^{n_{i}}\\] 无放回取样 (超几何分布): \\[P(\\left\\{ p_{i}, n_{i} \\right\\}) = \\frac{&prod; \\left(\\begin{matrix} N_{i} &#92;&#92; n_{i} \\end{matrix}\\right)}{\\left(\\begin{matrix} &sum; N_{i} &#92;&#92; &sum; n_{i} \\end{matrix}\\right)}\\] Definition: \\(\\left(\\begin{matrix} N &#92;&#92; n \\end{matrix}\\right) = \\frac{N &times; (N - 1) &times; \\cdots &times; (n+1)}{n &times; \\cdots &times; 1}\\) 条件概率 \\(P(B|A) = \\frac{P(A &and; B)}{P(A)}\\) 事件独立性 Definition: \\(P(A B) = P(A) P(B) &hArr; P(B|A) = P(B) &hArr;\\) A, B 独立 Application 判断随机变量的独立性 设随机变量 \\(X, Y\\) 的 PDF 为 \\(f(x, y) = A \\mathrm{e}^{- a x^2 + b x y - c y^2}, - &infin; &lt; x &lt; &infin;, - &infin; &lt; y &lt; &infin;\\), 问在什么条件下 \\(X\\) 与 \\(Y\\) 相互独立? 证明需要证明 \\(f(x, y) = f_{X}(x) f_{Y}(y)\\) 概率计算 边沿概率, 全概率公式, 贝叶斯公式 边沿概率 Application: 已知联合分布, 计算边缘分布 补全联合分布 全概率公式 \\(P(A) = &sum; P(A | B_{i}) P(B_{i})\\), 其中 \\(\\left\\{ B_{i} \\right\\}\\) 为样本空间的划分 贝叶斯公式 \\(P(B_{i} | A) = \\frac{P(A|B_{i})P(B_{i})}{&sum;_{j=1}^{n} P(A|B_{j}) P(B_{j})}\\) Memonic: \\(P(B_{i} | A) P(A) = P(B_{i} &and; A) = P(A | B_{i}) P(B_{i})\\) Application: 误判概率计算 (逆概率计算) 已知某放射源产生的射线中包含 A, B, C 三种粒子, 占比分别 1/2, 1/6, 1/3. 在实验的粒子鉴别过程中, A 粒子被误判为其他粒子的概率为 10%, B, C 粒子被误判为 A 粒子的概率分别为 12% 和 21%. 请计算出被鉴别为 A 粒子的事例来自真实的 A 粒子的概率. 已知 \\(P(S_{A})\\) (\\(S_{*}\\) 对样本空间的划分) 以及 \\(P(D_{A} | S_{ *})\\) 求 \\(P(S_{A} | D_{A}) = \\frac{P(D_{A} | S_{A}) P(S_{A})}{&sum; P(D_{A}|S_{})P(S_{})\\) 随机变量及其分布 随机变量 为了和之后的子样 (\\(x_{i}\\)) 作区分, 这里用 \\(X\\) 表示随机变量 数学符号一坨乱麻 不知道是不是 PPT 里面的数学公式和文本是东拼西凑的, 里面的数学公式符号非常的不一致&#8230; 学起来很混乱. 概率密度函数 (PDF): \\(f(X)\\) 累计分布函数 (CDF): \\(F(X) = &int; f(X) \\mathrm{d}x\\) 随机变量的映射: Algorithm I: \\(F_{y}(y) = P(g(x) \\leq y)\\) Algorithm II: \\(g\\) 为映射作用在随机变量 \\(X\\) 上, 则 \\(f_{Y}(Y) = f_{X}(g^{-1}(Y)) \\left| \\frac{\\mathrm{d}}{\\mathrm{d} Y} g^{-1}(Y) \\right|\\). Memonic: \\(F_{Y}(Y) = &int; f_{Y}(Y) \\mathrm{d} Y = &int; f_{X}(X) \\mathrm{d} X \\frac{\\mathrm{d} X}{\\mathrm{d} Y} &rArr; f_{Y} = f_{X}(g^{-1}(Y)) \\frac{\\mathrm{d} X}{\\mathrm{d} Y}\\) Application: 计算概率密度函数映射 随机变量 \\(X\\) 的概率密度函数为: \\[f(x) = \\left\\{ \\begin{matrix} x^2 / 18 &amp; -3 &lt; x &lt; 3 &#92;&#92; 0 &amp; \\mathrm{other} \\end{matrix} \\right.\\] 求 \\(Y = (X + 1)^2\\) 的概率密度函数. 非单调映射 \\(F_{Y}(y) = P(g(X) \\leq y)\\) 计算变换关系 已知 \\(X, Y\\) 为 \\((0, 1)\\) 区间上均匀分布的随机变量且相互独立, 二维随机变量 \\(U, V\\) 和 \\(X, Y\\) 之间存在变换关系: \\(U = cos (2 &pi; x) \\sqrt{- 2 ln y}, V = sin (2 &pi; x) \\sqrt{- 2 ln y}\\), 证明 \\(U, V\\) 服从 \\(N(0, 1)\\). 映射的数字特征 随机变量 \\(X, Y\\) 都服从 \\(N(&mu;, &sigma;^2)\\) 且相互独立, 令 \\(U = a X + b Y, W = a X b Y\\), 求 \\(U\\) 与 \\(W\\) 的相关系数 \\(&rho;_{UW}\\). \\(E(a X + b Y) = a E(X) + b E(Y)\\) \\(E(X Y) = E(X) E(Y)\\) \\(\\mathrm{cov}(a X + b Y, Z) = a \\mathrm{cov}(X, Z) + b \\mathrm{cov}(Y, Z)\\) 随机变量的数字特征 期望值 \\(E(g(X)) = &int; g(X) f(X) \\mathrm{d} X\\) Linear Property: \\(E(&sum; a_{i} X_{i}) = a_{i} E(X_{i})\\) 中位数 (\\(p\\) 分位数) 最可几值 (\\(x |_{P(x) &rarr; \\mathrm{maximum}}\\)) 矩 \\(&alpha;_{l} = E((X - C)^{l})\\) 一阶原点矩 \\(&mu; = E(X)\\) 二阶中心矩 \\(&sigma;^{2} = E(X - &mu;)^{2} = V(X)\\) Quick Calc: \\(V(&sum; a_{i} X_{i}) = &sum; a_{i}^{2} V(X_{i}) + 2 &sum; a_{i} a_{j} \\mathrm{cov}(X_{i}, X_{j})\\) 偏度 \\(&gamma;_{1} = \\frac{&mu;_{3}}{&mu;_{2}^{3/2}} = \\frac{E((X - &mu;)^{3})}{&sigma;^{3}}\\) 随机变量对其均值的不对称程度, 偏斜程度 峰度 \\(&gamma;_{2} = \\frac{&mu;_{4}}{&mu;_{2}^{2}} - 3 = \\frac{E((X - &mu;)^{4})}{&sigma;^{4}} - 3\\) 概率密度的尖锐程度于正态分布概率曲线尖锐程度的对比 切比雪夫不等式 \\(P(\\left| X - &mu; \\right| \\geq &epsilon;) \\leq \\frac{&sigma;^{2} }{&epsilon;^{2} }\\) GPT: 不关心随机变量服从什么具体的分布, 只要知道期望 (均值) 和方差, 就能给概率划定一个界限. 令 \\(&epsilon; = k &sigma;\\), 即 \\(P(|X - &mu;| \\geq k &sigma;) \\leq \\frac{1}{k^{2}}\\), 限定了和均值偏差大于 \\(k &sigma;\\) 的概率的上界. GPT 感觉课件里面一堆乱七八糟的东西, 有点像是只言片语的梦话连不成线; 或者是有用的信息在长长的文字和定义证明中失去了信息的传递能力. 低情商发言就是: 不如 AI. 以后老师不如就给大纲和要点, 让学生问 AI 算了. 随机变量的特征函数 \\(\\varphi_{X}(t) = E(e^{i t X})\\) GPT: 等价于傅里叶变换为频域. \\(&lambda;_{n} = E(X^{n}) = i^{-n} \\left[ \\frac{\\mathrm{d}^{n} \\varphi_{X}(t)}{\\mathrm{d} t^{n}} \\right]\\) 随机变量的分布 多项式分布 泊松分布 均匀分布 正态分布 多维随机变量及其分布 二维随机变量的分布, 独立性 \\(F(x, y) = P(X \\leq x &and; Y \\leq y)\\) 独立性 Definition: \\(f(x, y) = f_{X}(x) f_{Y}(y)\\) 条件概率分布 二维随机变量的数字特征 协方差 \\(\\mathrm{cov}(X, Y) = E((X - &mu;_{X})(Y - &mu;_{Y})) = E(XY) - E(X) E(Y)\\) 相关系数 \\(&rho;_{XY} = \\frac{\\mathrm{cov}(X, Y)}{&sigma;_{X} &sigma;_{Y}}\\) Memonic: \\(\\mathrm{cov}(X, X) = &sigma;_{X}^{2}\\) 协方差矩阵 \\(V_{ij} = \\mathrm{cov}(X_{i}, X_{j})\\) 两个随机变量的函数的分布 多维随机变量, 向量和矩阵记号 随机变量分布 中心极限定理 GPT: 样本量 \\(n\\) 足够大, 均值分布趋向于正态分布. Application: 二项分布的正态近似 &#8230; 子样及分布 随机子样, 子样分布函数 子样 子样空间 子样分布函数 统计量及其数字特征 子样中位数 子样平均: \\(\\bar{x} = \\frac{1}{n} &sum; x_{i}\\) 子样方差: \\(S^{2} = \\frac{1}{n-1} &sum; (x_{i} - \\bar{x})^{2} = \\frac{1}{n - 1} \\left( (&sum; x_{i}^{2})- n \\bar{x}^{2} \\right)\\) 子样标准值: \\(S\\) 子样协方差: \\(S_{xy} = \\frac{1}{n-1} &sum; (x_{i} - x) (y_{i} - y) = \\frac{1}{n-1} \\left( &sum; x_{i} y_{i} - n \\bar{x} \\bar{y} \\right)\\) 子样相关系数: \\(&rho;_{xy} = \\frac{S_{XY}}{S_{X} S_{Y}} = \\frac{&sum; (x_{i} - \\bar{x})(y_{i} - \\bar{y})}{(&sum; x_{i}^{2} - n \\bar{x}^{2} )^{1/2} (&sum; y_{i}^{2} - n \\bar{y}^{2})^{1/2}}\\) Memonic: 和随机变量一样, 但是平均的项变为 \\(1/(n-1)\\). Application 根据子样数据计算数字特征 抽样分布 \\(&chi;^{2}\\) 分布: 总体服从正态分布时, 样本方差与总体方差的比值服从卡方分布 \\(t\\) 分布: 样本均值于标准误差的比值服从 t 分布 \\(F\\) 分布: 两个样本方差的比值 Application 已知子样总体, 计算统计量的概率分布 抽样数据的图形表示, 概率分布 参数估计的一般概念 估计量, 似然函数 区间估计 Algorithm: Application: 单正态总体的均值 单正态总体的方差 双正态总体的均值差 正态总体方差的置信区间 Algorithm Application 已知样本总体, 给定置信水平, 计算置信区间 估计对象已知条件置信区间公式 均值 \\(&mu;\\)\\(&sigma;^{2}\\) 已知\\(\\left[ \\bar{x} \\mp Z_{&alpha;/2} \\frac{&sigma;}{\\sqrt{n}} \\right]\\) 均值 \\(&mu;\\)\\(&sigma;^{2}\\) 未知\\(\\left[ \\bar{x} \\mp t_{&alpha;/2}(n-1) \\frac{S}{\\sqrt{n}} \\right]\\) 方差 \\(&sigma;^{2}\\)\\(&mu;\\) 未知\\(\\left[ \\frac{(n-1) S^2}{&chi;_{&alpha;/2}^{2}(n-1)}, \\frac{(n-1) S^{2}}{&chi;_{1-&alpha;/2}^{2} (n-1)} \\right]\\) Calculation: \\(Z_{&alpha;/2} = &Phi;^{-1}(1 - &alpha;/2)\\), where \\(&Phi;\\) 为 \\(N(0, 1)\\) 正态分布 \\(t_{&alpha;/2}(n)\\) 为自由度为 \\(n-1\\) 的 Student 分布 \\(&chi;_{&alpha;/2}^{2}(n)\\), where \\(&chi;^{2}(n)\\) 为自由度为 \\(n-1\\) 的卡方分布 明明是查表才能知道的东西&#8230; 极大似然法 极大似然原理 Algorithm: 构造函数: \\(L(\\left\\{ x_{i} \\right\\} | &theta;) = &prod; f(x_{i}|&theta;)\\) 极大似然条件约束下求解 \\(&theta;\\): \\(&part;_{&theta;} ln L = 0 &and; &part;_{&theta;}^{2} ln L &lt; 0\\) Application: 参数区间估计 (似然区间) Algorithm: \\(&gamma; = \\frac{&int;_{&theta;_{a}}^{&theta;_{b}} L(X|&theta;) \\mathrm{d}&theta;}{&int;_{-&infin;}^{&infin;} L(X|&theta;) \\mathrm{d}&theta;}\\) 同上, 在 \\(&part;_{&theta;} &gamma; = 0 &and; &part;_{&theta;}^{2} &gamma; &lt; 0\\) 条件下求解 子样观测值 几何分布 设 \\(x_{1}, \\cdots, x_{n}\\) 是几何分布总体的子样观测值, 其分布律为 \\(P(X = x) = p (1 - p)^{x-1}, x = 1, 2, \\cdots, &infin;\\). 求参数 \\(p\\) 的极大似然估计. \\(L(\\left\\{ x_{i} \\right\\} | p) = &prod; p(1-p)^{x-1}\\) \\(&part;_{p} ln L = &part;_{p} ln \\left( p^{n} (1-p)^{&sum; x_{i} - n} \\right) = \\frac{n - p &sum; x_{i}}{p(1-p)}\\) 极大似然法应用于多个实验结果的合并 \\(&mu; = \\frac{&sum; \\frac{&mu;_{i}}{&sigma;_{i}^{2}}}{ &sum; \\frac{1}{ &sigma;_{i}^{2 }} } \\) \\(V = \\frac{1}{&sum; \\frac{1}{&sigma;_{i}^{2} }}\\) 极大似然法用于直方图 最小二乘法 最小二乘拟合 Algorithm: 构造误差函数 \\(\\mathrm{err}(&theta;) = &sum; (y_{i} - f(x_{i} | &theta;))^{2}\\) 最小化误差 \\(&part;_{&theta;} \\mathrm{err} = 0 &and; &part;_{&theta;}^{2} \\mathrm{err} &gt; 0\\) 最小二乘用于直方图数据 假设检验 原假设和备择假设 参数检验: 根据观测值检验参数是否等于某个给定值 非参数检验: 根据观测值检验模型函数是否有某个特定函数形式 原假设: 要验证的假设为原假设 备择假设 假设检验的一般方法 Algorithm 原假设和备择假设 (Input) 原假设成立时的统计量 计算统计量观测值 计算拒绝域 判断统计决策 Application 已知正态分布 (子样分布) 和方差 某厂生产的一种电池, 其寿命长期以来服从方差 \\(&sigma;^{2} = 5000\\ \\mathrm{h}^{2}\\) 的正态分布. 近期生产一批这种电池, 从生产的情况来看不能肯定寿命方差是否改变. 现随机地取 26 个电池, 测得寿命的样本方差为 \\(S^{2} = 9200\\ \\mathrm{h}^{2}\\). 问根据这一数据能否推断这批电池寿命方差较以往有显著变换 (取 \\(&alpha; = 0.02\\))? 正态总体的参数检验 拟合优度检验 \\(&chi;^{2}\\) 检验 Definition: \\(&chi;^{2} = &sum; \\frac{(x_{i} - X)^{2}}{X}\\) 似然比检验 信号的统计显著性 贝叶斯公式 Monte Carlo 方法 实际使用过程中, Monte Carlo 方法需要很好地构造随机过程&#8230; 模式识别系统 乐, 只有大段文字, 仿佛在看本科生大学写作课程作业的综述论文. (而且都还很老, 并且不知道为啥, 物理系这边的 AI 课特别喜欢讲 Hopfield 网络, 哪怕他们讲的 Hopfield 网络都不太能用, 或者说不知道该怎么用. ) 理论课的通病 不知道为啥, 讲理论课的老师往往会把定理/算法/规律的严格证明视作是实现的正确性, 好像是只要我证明了我的做法是对的, 那么做的结果就一定是对的. 然后对于如何评价实际结果的正确性的方法几乎不提. 搞清楚课程定位啊淦, 这课不是 物理 中的概率统计吗? 不是实验类的课程吗? 别告诉我说你们实验组有祖传代码拟合分析误差自动生成论文配图, 只需要知道自己的原理是对的是吧&#8230; (当然, 假设检验确实是一个可用的方法, 但是什么叫做假设检验课在证明假设检验是对的? ) 又, 还有的困难是引入的新理论和旧的理论接不上, 有种粘连的感觉. 这有种讲分析力学讲了半天的拉格朗日方程, 结果最后发现用的是牛顿三定律一样的奇怪. 应该就直接抛弃旧的知识, 用新的视角去讲整个 (要么就干脆承认, 说我们不讲新的). 在本科赵爹的课上的那个做法就很不错, 用新的视角去引入 (上来就是对称性守恒律, 相信基础教育的牛顿三定律的普及教育, 而不是重新教一遍牛顿三定律再开始讲能量视角), 然后在打好了新的视角的基础之后, 再回过头去审视旧的理论如何迁移到新的视角里. 比如在概统的这边, 我觉得确实可以用测度空间的方法来解释很多东西, 然后再回过头去讲其他东西, 就会很不错. 不过可能也要看具体的应用或者教学需求, 用测度论来讲实验误差分析的话, 感觉不是很好理解 (也有可能只是我不会, 之后可以重新学), 但是如果直接从实验误差为主线来重新讲课, 那估计就能够砍掉很多的垃圾学时, 并且还能更有条理&#8230; 特征提取和选择 效率, 误判率, 分辨能力 效率: 正确选定一个信号事例的效率 \\(&epsilon;_{SS} = &int; f_{S}(y) \\mathrm{d}y\\) 误判率: 将本底错误选择为一个信号事例的误判率 \\(&epsilon;_{SB} = &int; f_{B}(y) \\mathrm{d}y\\) 误判率: 效率与误判率的比 \\(r = &epsilon;_{SS} / &epsilon;_{SB}\\) Memonic: 引入误差矩阵 判断 \\ 实际真假 真真真真假 假假真假假 效率: 真真 / (真真 + 假真) 误判率: 真假 / (真假 + 假假) 方差: \\(V(&epsilon;_{SS}) = \\frac{&epsilon;_{SS} (1 - &epsilon;_{SS})}{N_{S}}\\) \\(V(r)/r^{2} = V(&epsilon;_{SS}) / &epsilon;_{SS}^{2} + V(&epsilon;_{SB}) / &epsilon;_{SB}^{2}\\) 方法 贝叶斯决策 线性判别 决策树方法 人工神经网络 近邻法 概率密度估计量方法 H 矩阵判别 函数判别 SVM 小信号测量的区间估计 测量误差及其分类 和子样误差放在一起 课上把测量误差和子样误差分开来讲了, 实际上我觉得这两个东西应当放在一起讲, 毕竟这两个才是真的相关的知识, 也是在实验里真的用的东西. 明明可以用一个实验数据的采集的全流程来讲随机子样, 子样分布函数这些知识, 非要用概念朗诵, 鼠标扫扫公式 &#8211; 大家要记住哦, 很重要哦, 这种方式来教课&#8230; 不过也可以理解就是了 &#8211; who cares? Application: 误差传递公式 探测器效率计算: 实际计数 / 源计数 Application: 多层符合探测效率 m 层中至少 k 触发: \\(&epsilon;_{\\mathrm{tol}} = &sum; \\left(\\begin{matrix}m &#92;&#92; k\\end{matrix}\\right) &epsilon;^{i} (1 - &epsilon;)^{m-i}\\)","headline":"概统","mainEntityOfPage":{"@type":"WebPage","@id":"/notes/stastics/"},"url":"/notes/stastics/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css">

  <style type="text/css">
    img {
      margin-left: auto; 
      margin-right:auto; 
      display:block;
    }
  </style><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="My Blog" /><script>
  document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
        // customised options
        // • auto-render specific keys, e.g.:
        delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: '\\(', right: '\\)', display: false},
            {left: '\\[', right: '\\]', display: true}
        ],
        // • rendering keys, e.g.:
        throwOnError : false
      });
  });
</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.23/dist/katex.min.css" integrity="sha384-z91AFMXXGZasvxZz5DtKJse3pKoTPU0QcNFj/B4gDFRmq6Q2bi1StsT7SOcIzLEN" crossorigin="anonymous">

<!-- The loading of KaTeX is deferred to speed up page rendering -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.23/dist/katex.min.js" integrity="sha384-Af7YmksQNWRLMvro3U9F84xa0paoIu7Pu2niAIUmZoI09Q4aCsbha5dvaj1tHy6K" crossorigin="anonymous"></script>

<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.23/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">My Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/categories/">Categories</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">概统</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2026-01-19T00:00:00+00:00" itemprop="datePublished">Jan 19, 2026
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1>About</h1>
<details><summary>挺一般的研究生课程</summary>
<p>评价是纯纯的 PPT 朗诵.</p>
</details>
<h1>Memonic</h1>
<ul>
  <li>贝叶斯公式 \(P(B_{i} | A) = \frac{P(A|B_{i})P(B_{i})}{&sum;_{j=1}^{n} P(A|B_{j}) P(B_{j})}\)</li>
  <li>随机变量映射 \(F_{Y}(Y) = P(g(X) \leq Y)\)</li>
  <li>统计量数字特征计算
    <table>
      <tr><th>特征</th><th>随机变量</th><th>子样</th></tr>
      <tr><td>平均</td><td>\(\frac{1}{n} &sum; x_i\)</td><td>\(\frac{1}{n} &sum; x_{i}\)</td></tr>
      <tr><td>方差</td><td>\(&sigma;^{2} = &sum; (X - \bar{X})^{2} = &sum; X^{2} - (&sum; X)^{2}\)</td><td>\(S^{2} = \frac{1}{n-1} &sum; (x_{i} - \bar{x}_{i})^{2} = \frac{1}{n-1} ((&sum; x_{i}^{2}) - n \bar{x}^{2})\)</td></tr>
      <tr><td>多元方差</td><td>\(V(&sum; a_{i} X_{i}) = &sum; a_{i}^{2} V(X_{i}) + &sum; a_{i} a_{j} \mathrm{cov}(X_{i}, X_{j})\)</td><td>-</td></tr>
      <tr><td>协方差</td><td>\(\mathrm{cov}(X, Y) = E((X - E(X))(Y - E(Y))) = \mathrm{cov}(X, Y) = E(XY) - E(X)E(Y)\)</td><td>\(S_{XY} = \frac{1}{n-1} &sum; (x_{i} - \bar{x})(y_{i} - \bar{y})\)</td></tr>
      <tr><td>相关系数</td><td>\(&rho;_{XY} = \mathrm{cov}(X, Y) / (&sigma;_{X} &sigma;_{Y})\)</td><td>\(r = \frac{&sum; (x_{i} - \bar{x})(y_{i} - \bar{y})}{\sqrt{(x_{i} - \bar{x})} \sqrt{(y_{i} - \bar{y})}}\)</td></tr>
    </table>
  </li>
  <li>实验数据合并
    <p>\[&mu; = \frac{&sum; \frac{&mu;_{i}}{&sigma;_{i}^{2}}}{ &sum; \frac{1}{ &sigma;_{i}^{2 }} },\ V = \frac{1}{&sum; \frac{1}{&sigma;_{i}^{2} }}\]</p>
  </li>
  <li>极大似然法
    <p>\[L = &prod; f(x_{i}|&theta;),\ &part;_{&theta;} ln L = 0 &and; &part;_{&theta;}^{2} ln L &lt; 0\]</p>
  </li>
  <li>最小二乘法
    <p>\[\mathrm{err}(&theta;) = &sum; (y_{i} - f(x_{i}|&theta;))^{2},\ &part;_{&theta;} \mathrm{err} = 0 &and; &part;_{&theta;}^{2} \mathrm{err} &gt; 0\]</p>
  </li>
  <li>假设检验
    <table>
      <tr><th>条件</th><th>检验对象</th><th>统计量观测值</th><th>拒绝域</th></tr>
      <tr><td>\(&sigma;^{2}\) 已知</td><td>均值 \(&mu;\)</td><td>\(Z = \frac{\bar{X} - &mu;_{0}}{&sigma; / \sqrt{n}}\)</td><td>\(\left\vert Z \right\vert &gt; Z_{&alpha;/2}(n-1)\)</td></tr>
      <tr><td>\(&sigma;^{2}\) 未知</td><td>均质 \(&mu;\)</td><td>\(t = \frac{\bar{X} - &mu;_{0}}{S / \sqrt{n}}\)</td><td>\(\left\vert t \right\vert &gt; t_{&alpha;/2}(n-1)\)</td></tr>
      <tr><td></td><td>方差 \(&sigma;^{2}\)</td><td>\(&chi;^{2} = \frac{(n-1) S^{2}}{&sigma;_{0}^{2}}\)</td><td>\(&chi;^{2} &lt; &chi;_{1-&alpha;/2}^{2}(n-1) &or; &chi;^{2} &gt; &chi;_{&alpha;/2}^{2}(n-1)\)</td></tr>
    </table>
  </li>
  <li>几个分布
    <table>
      <tr><th>分布</th><th>PDF</th><th>E(X)</th><th>V(X)</th><th>Notes</th></tr>
      <tr><td>二项分布 \(B(n, p)\)</td><td>\(\left(\begin{matrix} n &#92;&#92; k\end{matrix}\right) p^{k} (1-p)^{n-k}\)</td><td>\(np\)</td><td>\(np(1-p)\)</td><td>(多项式分布)</td></tr>
      <tr><td>泊松分布 \(P(&lambda;)\)</td><td>\(\frac{&lambda;^{k} \mathrm{e}^{-&lambda;}}{k!}\)</td><td>\(&lambda;\)</td><td>\(&lambda;\)</td><td>粒子计数标准差 \(\sqrt{N}\)</td></tr>
      <tr><td>正态分布 \(N(&mu;,&sigma;^{2})\)</td><td>\(\frac{1}{\sqrt{2 &pi;} &sigma;} \mathrm{exp}\left( - \frac{(x-&mu;)^{2}}{2&sigma;^{2}} \right)\)</td><td>\(&mu;\)</td><td>\(&sigma;\)</td><td></td></tr>
    </table>
  </li>
</ul>
<h1>概率论初步</h1>
<h2>随机试验, 随机事件, 样本空间</h2>
<details><summary>GPT: 用一句话解释名词概念</summary>
<ul>
  <li>必然现象: 在特定条件下必定发生的现象, 任何情况下都会发生</li>
  <li>随机现象: 其结果不可预知, 受随机因素影响的现象</li>
  <li>随机试验: 可以重复进行且结果不确定的实验或过程</li>
  <li>互相独立: 两个或多个事件的发生与否互不影响</li>
  <li>随机事件: 随机试验中可能发生的结果或现象</li>
  <li>基本事件: 在随机试验中不再可以分解的最基本的事件</li>
  <li>样本空间: 随机试验中所有可能基本事件的集合</li>
  <li>元素: 样本空间中的每一个基本事件</li>
  <li>必然事件: 一定会发生的事件, 其概率为 1</li>
  <li>不可能事件: 一定不会发生的事件, 其概率为 0</li>
</ul>
</details>
<ul>
  <li>事件关系</li>
  <li>事件运算 \(A &and; B\), \(A &or; B\), \(&not; A\)</li>
</ul>
<h2>概率</h2>
<p><b>Application</b></p>
<ul>
  <li>多项式分布
    <blockquote>
      <p>盒子里有 5 个红球, 4 个白球和 3 个蓝球,
        从盒子里随机选一球, 记下颜色, 然后返回盒子里.
        求挑选 6 次后出现 3 红, 2 白, 1 蓝的概率.</p>
    </blockquote>
    <ul>
      <li>有放回取样 (多项式分布):
        <p>\[P(\left\{ p_{i}, n_{i} \right\}) = \frac{(&sum; n_{i})!}{&prod; n_{i}!} &prod; p_{i}^{n_{i}}\]</p>
      </li>
      <li>无放回取样 (超几何分布):
        <p>\[P(\left\{ p_{i}, n_{i} \right\}) = \frac{&prod; \left(\begin{matrix} N_{i} &#92;&#92; n_{i} \end{matrix}\right)}{\left(\begin{matrix} &sum; N_{i} &#92;&#92; &sum; n_{i} \end{matrix}\right)}\]</p>
        <p><b>Definition</b>: \(\left(\begin{matrix} N &#92;&#92; n \end{matrix}\right) = \frac{N &times; (N - 1) &times; \cdots &times; (n+1)}{n &times; \cdots &times; 1}\)</p>
      </li>
    </ul>
  </li>
</ul>
<h2>条件概率</h2>
<ul>
  <li>\(P(B|A) = \frac{P(A &and; B)}{P(A)}\)</li>
  <li>事件独立性
    <p><b>Definition</b>:
      \(P(A B) = P(A) P(B) &hArr; P(B|A) = P(B) &hArr;\) A, B 独立</p>
    <p><b>Application</b></p>
    <ul>
      <li>判断随机变量的独立性
        <blockquote>
          <p>设随机变量 \(X, Y\) 的 PDF 为
            \(f(x, y) = A \mathrm{e}^{- a x^2 + b x y - c y^2}, - &infin; &lt; x &lt; &infin;, - &infin; &lt; y &lt; &infin;\),
            问在什么条件下 \(X\) 与 \(Y\) 相互独立?</p>
        </blockquote>
        <ul>
          <li>证明需要证明 \(f(x, y) = f_{X}(x) f_{Y}(y)\)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<h2>概率计算</h2>
<h2>边沿概率, 全概率公式, 贝叶斯公式</h2>
<ul>
  <li>边沿概率
    <p><b>Application</b>:</p>
    <ul>
      <li>已知联合分布, 计算边缘分布</li>
      <li>补全联合分布</li>
    </ul>
  </li>
  <li>全概率公式 \(P(A) = &sum; P(A | B_{i}) P(B_{i})\),
    其中 \(\left\{ B_{i} \right\}\) 为样本空间的划分</li>
  <li>贝叶斯公式 \(P(B_{i} | A) = \frac{P(A|B_{i})P(B_{i})}{&sum;_{j=1}^{n} P(A|B_{j}) P(B_{j})}\)
    <p><b>Memonic</b>:
      \(P(B_{i} | A) P(A) = P(B_{i} &and; A) = P(A | B_{i}) P(B_{i})\)</p>
    <p><b>Application</b>:</p>
    <ul>
      <li>误判概率计算 (逆概率计算)
        <blockquote>
          <p>已知某放射源产生的射线中包含 A, B, C 三种粒子,
            占比分别 1/2, 1/6, 1/3. 在实验的粒子鉴别过程中,
            A 粒子被误判为其他粒子的概率为 10%,
            B, C 粒子被误判为 A 粒子的概率分别为 12% 和 21%.
            请计算出被鉴别为 A 粒子的事例来自真实的 A 粒子的概率.</p>
        </blockquote>
        <ul>
          <li>已知 \(P(S_{A})\) (\(S_{*}\) 对样本空间的划分) 以及 \(P(D_{A} | S_{ *})\)
            <p>求 \(P(S_{A} | D_{A}) = \frac{P(D_{A} | S_{A}) P(S_{A})}{&sum; P(D_{A}|S_{<b>})P(S_{</b>})\)</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<h1>随机变量及其分布</h1>
<h2>随机变量</h2>
<ul>
  <li>为了和之后的子样 (\(x_{i}\)) 作区分, 这里用 \(X\) 表示随机变量
  <details><summary>数学符号一坨乱麻</summary>
    <p>不知道是不是 PPT 里面的数学公式和文本是东拼西凑的,
      里面的数学公式符号非常的不一致&#8230; 学起来很混乱.</p>
  </details>
  </li>
  <li>概率密度函数 (PDF): \(f(X)\)</li>
  <li>累计分布函数 (CDF): \(F(X) = &int; f(X) \mathrm{d}x\)</li>
  <li>随机变量的映射:
    <p><b>Algorithm I</b>: \(F_{y}(y) = P(g(x) \leq y)\)</p>
    <p><b>Algorithm II</b>:
      \(g\) 为映射作用在随机变量 \(X\) 上, 则 \(f_{Y}(Y) = f_{X}(g^{-1}(Y)) \left| \frac{\mathrm{d}}{\mathrm{d} Y} g^{-1}(Y) \right|\).</p>
    <p><b>Memonic</b>:
      \(F_{Y}(Y) = &int; f_{Y}(Y) \mathrm{d} Y = &int; f_{X}(X) \mathrm{d} X \frac{\mathrm{d} X}{\mathrm{d} Y} &rArr; f_{Y} = f_{X}(g^{-1}(Y)) \frac{\mathrm{d} X}{\mathrm{d} Y}\)</p>
    <p><b>Application</b>:</p>
    <ul>
      <li>计算概率密度函数映射
        <blockquote>
          <p>随机变量 \(X\) 的概率密度函数为:</p>
          <p>\[f(x) = \left\{ \begin{matrix} x^2 / 18 &amp; -3 &lt; x &lt; 3 &#92;&#92; 0 &amp; \mathrm{other} \end{matrix} \right.\]</p>
          <p>求 \(Y = (X + 1)^2\) 的概率密度函数.</p>
        </blockquote>
        <ul>
          <li>非单调映射 \(F_{Y}(y) = P(g(X) \leq y)\)</li>
        </ul>
      </li>
      <li>计算变换关系
        <blockquote>
          <p>已知 \(X, Y\) 为 \((0, 1)\) 区间上均匀分布的随机变量且相互独立,
            二维随机变量 \(U, V\) 和 \(X, Y\) 之间存在变换关系:
            \(U = cos (2 &pi; x) \sqrt{- 2 ln y}, V = sin (2 &pi; x) \sqrt{- 2 ln y}\),
            证明 \(U, V\) 服从 \(N(0, 1)\).</p>
        </blockquote>
      </li>
      <li>映射的数字特征
        <blockquote>
          <p>随机变量 \(X, Y\) 都服从 \(N(&mu;, &sigma;^2)\) 且相互独立, 令 \(U = a X + b Y, W = a X b Y\),
            求 \(U\) 与 \(W\) 的相关系数 \(&rho;_{UW}\).</p>
        </blockquote>
        <ul>
          <li>\(E(a X + b Y) = a E(X) + b E(Y)\)</li>
          <li>\(E(X Y) = E(X) E(Y)\)</li>
          <li>\(\mathrm{cov}(a X + b Y, Z) = a \mathrm{cov}(X, Z) + b \mathrm{cov}(Y, Z)\)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<h2>随机变量的数字特征</h2>
<ul>
  <li>期望值 \(E(g(X)) = &int; g(X) f(X) \mathrm{d} X\)
    <p><b>Linear Property</b>: \(E(&sum; a_{i} X_{i}) = a_{i} E(X_{i})\)</p>
  </li>
  <li>中位数 (\(p\) 分位数)</li>
  <li>最可几值 (\(x |_{P(x) &rarr; \mathrm{maximum}}\))</li>
  <li>矩 \(&alpha;_{l} = E((X - C)^{l})\)
    <ul>
      <li>一阶原点矩 \(&mu; = E(X)\)</li>
      <li>二阶中心矩 \(&sigma;^{2} = E(X - &mu;)^{2} = V(X)\)</li>
    </ul>
    <p><b>Quick Calc</b>:</p>
    <ul>
      <li>\(V(&sum; a_{i} X_{i}) = &sum; a_{i}^{2} V(X_{i}) + 2 &sum; a_{i} a_{j} \mathrm{cov}(X_{i}, X_{j})\)</li>
    </ul>
  </li>
  <li>偏度 \(&gamma;_{1} = \frac{&mu;_{3}}{&mu;_{2}^{3/2}} = \frac{E((X - &mu;)^{3})}{&sigma;^{3}}\)
    <p>随机变量对其均值的不对称程度, 偏斜程度</p>
  </li>
  <li>峰度 \(&gamma;_{2} = \frac{&mu;_{4}}{&mu;_{2}^{2}} - 3 = \frac{E((X - &mu;)^{4})}{&sigma;^{4}} - 3\)
    <p>概率密度的尖锐程度于正态分布概率曲线尖锐程度的对比</p>
  </li>
  <li>切比雪夫不等式 \(P(\left| X - &mu; \right| \geq &epsilon;) \leq \frac{&sigma;^{2} }{&epsilon;^{2} }\)
    <p><b>GPT</b>: 不关心随机变量服从什么具体的分布, 只要知道期望 (均值) 和方差,
      就能给概率划定一个界限.</p>
    <p>令 \(&epsilon; = k &sigma;\), 即 \(P(|X - &mu;| \geq k &sigma;) \leq \frac{1}{k^{2}}\),
      限定了和均值偏差大于 \(k &sigma;\) 的概率的上界.</p>
  <details><summary>GPT</summary>
    <p>感觉课件里面一堆乱七八糟的东西, 有点像是只言片语的梦话连不成线;
      或者是有用的信息在长长的文字和定义证明中失去了信息的传递能力.</p>
    <p>低情商发言就是: 不如 AI.</p>
    <p>以后老师不如就给大纲和要点, 让学生问 AI 算了.</p>
  </details>
  </li>
</ul>
<h2>随机变量的特征函数</h2>
<ul>
  <li>\(\varphi_{X}(t) = E(e^{i t X})\)
    <p><b>GPT</b>: 等价于傅里叶变换为频域.</p>
  </li>
  <li>\(&lambda;_{n} = E(X^{n}) = i^{-n} \left[ \frac{\mathrm{d}^{n} \varphi_{X}(t)}{\mathrm{d} t^{n}} \right]\)</li>
</ul>
<h2>随机变量的分布</h2>
<ul>
  <li>多项式分布</li>
  <li>泊松分布</li>
  <li>均匀分布</li>
  <li>正态分布</li>
</ul>
<h1>多维随机变量及其分布</h1>
<h2>二维随机变量的分布, 独立性</h2>
<ul>
  <li>\(F(x, y) = P(X \leq x &and; Y \leq y)\)</li>
  <li>独立性
    <p><b>Definition</b>: \(f(x, y) = f_{X}(x) f_{Y}(y)\)</p>
  </li>
</ul>
<h2>条件概率分布</h2>
<h2>二维随机变量的数字特征</h2>
<ul>
  <li>协方差 \(\mathrm{cov}(X, Y) = E((X - &mu;_{X})(Y - &mu;_{Y})) = E(XY) - E(X) E(Y)\)</li>
  <li>相关系数 \(&rho;_{XY} = \frac{\mathrm{cov}(X, Y)}{&sigma;_{X} &sigma;_{Y}}\)
    <p><b>Memonic</b>: \(\mathrm{cov}(X, X) = &sigma;_{X}^{2}\)</p>
  </li>
  <li>协方差矩阵 \(V_{ij} = \mathrm{cov}(X_{i}, X_{j})\)</li>
</ul>
<h2>两个随机变量的函数的分布</h2>
<h2>多维随机变量, 向量和矩阵记号</h2>
<h1>随机变量分布</h1>
<h1>中心极限定理</h1>
<p><b>GPT</b>: 样本量 \(n\) 足够大, 均值分布趋向于正态分布.</p>
<p><b>Application</b>:</p>
<ul>
  <li>二项分布的正态近似
    <p>&#8230;</p>
  </li>
</ul>
<h1>子样及分布</h1>
<h2>随机子样, 子样分布函数</h2>
<ul>
  <li>子样</li>
  <li>子样空间</li>
  <li>子样分布函数</li>
</ul>
<h2>统计量及其数字特征</h2>
<ul>
  <li>子样中位数</li>
  <li>子样平均: \(\bar{x} = \frac{1}{n} &sum; x_{i}\)</li>
  <li>子样方差: \(S^{2} = \frac{1}{n-1} &sum; (x_{i} - \bar{x})^{2} = \frac{1}{n - 1} \left( (&sum; x_{i}^{2})- n \bar{x}^{2} \right)\)</li>
  <li>子样标准值: \(S\)</li>
  <li>子样协方差: \(S_{xy} = \frac{1}{n-1} &sum; (x_{i} - x) (y_{i} - y) = \frac{1}{n-1} \left( &sum; x_{i} y_{i} - n \bar{x} \bar{y} \right)\)</li>
  <li>子样相关系数: \(&rho;_{xy} = \frac{S_{XY}}{S_{X} S_{Y}} = \frac{&sum; (x_{i} - \bar{x})(y_{i} - \bar{y})}{(&sum; x_{i}^{2} - n \bar{x}^{2} )^{1/2} (&sum; y_{i}^{2} - n \bar{y}^{2})^{1/2}}\)</li>
</ul>
<p><b>Memonic</b>: 和随机变量一样, 但是平均的项变为 \(1/(n-1)\).</p>
<p><b>Application</b></p>
<ul>
  <li>根据子样数据计算数字特征</li>
</ul>
<h2>抽样分布</h2>
<ul>
  <li>\(&chi;^{2}\) 分布: 总体服从正态分布时, 样本方差与总体方差的比值服从卡方分布</li>
  <li>\(t\) 分布: 样本均值于标准误差的比值服从 t 分布</li>
  <li>\(F\) 分布: 两个样本方差的比值</li>
</ul>
<p><b>Application</b></p>
<ul>
  <li>已知子样总体, 计算统计量的概率分布</li>
</ul>
<h2>抽样数据的图形表示, 概率分布</h2>
<h1>参数估计的一般概念</h1>
<h2>估计量, 似然函数</h2>
<h2>区间估计</h2>
<p><b>Algorithm</b>:</p>
<p><b>Application</b>:</p>
<ul>
  <li>单正态总体的均值</li>
  <li>单正态总体的方差</li>
  <li>双正态总体的均值差</li>
</ul>
<h2>正态总体方差的置信区间</h2>
<p><b>Algorithm</b></p>
<p><b>Application</b></p>
<ul>
  <li>已知样本总体, 给定置信水平, 计算置信区间
    <table>
      <tr><th>估计对象</th><th>已知条件</th><th>置信区间公式</th></tr>
      <tr><td>均值 \(&mu;\)</td><td>\(&sigma;^{2}\) 已知</td><td>\(\left[ \bar{x} \mp Z_{&alpha;/2} \frac{&sigma;}{\sqrt{n}} \right]\)</td></tr>
      <tr><td>均值 \(&mu;\)</td><td>\(&sigma;^{2}\) 未知</td><td>\(\left[ \bar{x} \mp t_{&alpha;/2}(n-1) \frac{S}{\sqrt{n}} \right]\)</td></tr>
      <tr><td>方差 \(&sigma;^{2}\)</td><td>\(&mu;\) 未知</td><td>\(\left[ \frac{(n-1) S^2}{&chi;_{&alpha;/2}^{2}(n-1)}, \frac{(n-1) S^{2}}{&chi;_{1-&alpha;/2}^{2} (n-1)} \right]\)</td></tr>
    </table>
    <p><b>Calculation</b>:</p>
    <ul>
      <li>\(Z_{&alpha;/2} = &Phi;^{-1}(1 - &alpha;/2)\), where \(&Phi;\) 为 \(N(0, 1)\) 正态分布</li>
      <li>\(t_{&alpha;/2}(n)\) 为自由度为 \(n-1\) 的 Student 分布</li>
      <li>\(&chi;_{&alpha;/2}^{2}(n)\), where \(&chi;^{2}(n)\) 为自由度为 \(n-1\) 的卡方分布</li>
    </ul>
    <p>明明是查表才能知道的东西&#8230;</p>
  </li>
</ul>
<h1>极大似然法</h1>
<h2>极大似然原理</h2>
<p><b>Algorithm</b>:</p>
<ol>
  <li>构造函数: \(L(\left\{ x_{i} \right\} | &theta;) = &prod; f(x_{i}|&theta;)\)</li>
  <li>极大似然条件约束下求解 \(&theta;\): \(&part;_{&theta;} ln L = 0 &and; &part;_{&theta;}^{2} ln L &lt; 0\)</li>
</ol>
<p><b>Application</b>:</p>
<ul>
  <li>参数区间估计 (似然区间)
    <p><b>Algorithm</b>:</p>
    <ol>
      <li>\(&gamma; = \frac{&int;_{&theta;_{a}}^{&theta;_{b}} L(X|&theta;) \mathrm{d}&theta;}{&int;_{-&infin;}^{&infin;} L(X|&theta;) \mathrm{d}&theta;}\)</li>
      <li>同上, 在 \(&part;_{&theta;} &gamma; = 0 &and; &part;_{&theta;}^{2} &gamma; &lt; 0\) 条件下求解</li>
    </ol>
  </li>
  <li>子样观测值
    <ul>
      <li>几何分布
        <blockquote>
          <p>设 \(x_{1}, \cdots, x_{n}\) 是几何分布总体的子样观测值,
            其分布律为 \(P(X = x) = p (1 - p)^{x-1}, x = 1, 2, \cdots, &infin;\).
            求参数 \(p\) 的极大似然估计.</p>
        </blockquote>
        <ul>
          <li>\(L(\left\{ x_{i} \right\} | p) = &prod; p(1-p)^{x-1}\)</li>
          <li>\(&part;_{p} ln L = &part;_{p} ln \left( p^{n} (1-p)^{&sum; x_{i} - n} \right) = \frac{n - p &sum; x_{i}}{p(1-p)}\)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<h2>极大似然法应用于多个实验结果的合并</h2>
<ul>
  <li>\(&mu; = \frac{&sum; \frac{&mu;_{i}}{&sigma;_{i}^{2}}}{ &sum; \frac{1}{ &sigma;_{i}^{2 }} } \)</li>
  <li>\(V = \frac{1}{&sum; \frac{1}{&sigma;_{i}^{2} }}\)</li>
</ul>
<h2>极大似然法用于直方图</h2>
<h1>最小二乘法</h1>
<h2>最小二乘拟合</h2>
<p><b>Algorithm</b>:</p>
<ol>
  <li>构造误差函数 \(\mathrm{err}(&theta;) = &sum; (y_{i} - f(x_{i} | &theta;))^{2}\)</li>
  <li>最小化误差 \(&part;_{&theta;} \mathrm{err} = 0 &and; &part;_{&theta;}^{2} \mathrm{err} &gt; 0\)</li>
</ol>
<h2>最小二乘用于直方图数据</h2>
<h1>假设检验</h1>
<h2>原假设和备择假设</h2>
<ul>
  <li>参数检验: 根据观测值检验参数是否等于某个给定值</li>
  <li>非参数检验: 根据观测值检验模型函数是否有某个特定函数形式</li>
  <li>原假设: 要验证的假设为原假设</li>
  <li>备择假设</li>
</ul>
<h2>假设检验的一般方法</h2>
<p><b>Algorithm</b></p>
<ol>
  <li>原假设和备择假设 (Input)</li>
  <li>原假设成立时的统计量</li>
  <li>计算统计量观测值</li>
  <li>计算拒绝域</li>
  <li>判断统计决策</li>
</ol>
<p><b>Application</b></p>
<ul>
  <li>已知正态分布 (子样分布) 和方差
    <blockquote>
      <p>某厂生产的一种电池, 其寿命长期以来服从方差 \(&sigma;^{2} = 5000\ \mathrm{h}^{2}\) 的正态分布.
        近期生产一批这种电池, 从生产的情况来看不能肯定寿命方差是否改变.
        现随机地取 26 个电池, 测得寿命的样本方差为 \(S^{2} = 9200\ \mathrm{h}^{2}\).
        问根据这一数据能否推断这批电池寿命方差较以往有显著变换 (取 \(&alpha; = 0.02\))?</p>
    </blockquote>
  </li>
</ul>
<h2>正态总体的参数检验</h2>
<h2>拟合优度检验</h2>
<ul>
  <li>\(&chi;^{2}\) 检验
    <p><b>Definition</b>:</p>
    <ul>
      <li>\(&chi;^{2} = &sum; \frac{(x_{i} - X)^{2}}{X}\)</li>
    </ul>
  </li>
  <li>似然比检验</li>
</ul>
<h2>信号的统计显著性</h2>
<h1>贝叶斯公式</h1>
<h1>Monte Carlo 方法</h1>
<p>实际使用过程中, Monte Carlo 方法需要很好地构造随机过程&#8230;</p>
<h1>模式识别系统</h1>
<p>乐, 只有大段文字, 仿佛在看本科生大学写作课程作业的综述论文.
  (而且都还很老, 并且不知道为啥, 物理系这边的 AI 课特别喜欢讲 Hopfield 网络,
  哪怕他们讲的 Hopfield 网络都不太能用, 或者说不知道该怎么用. )</p>
<details><summary>理论课的通病</summary>
<p>不知道为啥, 讲理论课的老师往往会把定理/算法/规律的严格证明视作是实现的正确性,
  好像是只要我证明了我的做法是对的, 那么做的结果就一定是对的.</p>
<p>然后对于如何评价实际结果的正确性的方法几乎不提.</p>
<p>搞清楚课程定位啊淦, 这课不是 <b>物理</b> 中的概率统计吗? 不是实验类的课程吗?
  别告诉我说你们实验组有祖传代码拟合分析误差自动生成论文配图,
  只需要知道自己的原理是对的是吧&#8230;</p>
<p>(当然, 假设检验确实是一个可用的方法, 但是什么叫做假设检验课在证明假设检验是对的? )</p>
<p>又, 还有的困难是引入的新理论和旧的理论接不上, 有种粘连的感觉.
  这有种讲分析力学讲了半天的拉格朗日方程, 结果最后发现用的是牛顿三定律一样的奇怪.
  应该就直接抛弃旧的知识, 用新的视角去讲整个 (要么就干脆承认, 说我们不讲新的).
  在本科赵爹的课上的那个做法就很不错, 用新的视角去引入 (上来就是对称性守恒律,
  相信基础教育的牛顿三定律的普及教育, 而不是重新教一遍牛顿三定律再开始讲能量视角),
  然后在打好了新的视角的基础之后, 再回过头去审视旧的理论如何迁移到新的视角里.</p>
<p>比如在概统的这边, 我觉得确实可以用测度空间的方法来解释很多东西,
  然后再回过头去讲其他东西, 就会很不错. 不过可能也要看具体的应用或者教学需求,
  用测度论来讲实验误差分析的话, 感觉不是很好理解 (也有可能只是我不会, 之后可以重新学),
  但是如果直接从实验误差为主线来重新讲课, 那估计就能够砍掉很多的垃圾学时,
  并且还能更有条理&#8230;</p>
</details>
<h2>特征提取和选择</h2>
<h2>效率, 误判率, 分辨能力</h2>
<ul>
  <li>效率: 正确选定一个信号事例的效率 \(&epsilon;_{SS} = &int; f_{S}(y) \mathrm{d}y\)</li>
  <li>误判率: 将本底错误选择为一个信号事例的误判率 \(&epsilon;_{SB} = &int; f_{B}(y) \mathrm{d}y\)</li>
  <li>误判率: 效率与误判率的比 \(r = &epsilon;_{SS} / &epsilon;_{SB}\)</li>
</ul>
<p><b>Memonic</b>: 引入误差矩阵</p>
<table>
  <tr><th>判断 \ 实际</th><th>真</th><th>假</th></tr>
  <tr><td>真</td><td>真真</td><td>真假</td></tr>
  <tr><td>假</td><td>假真</td><td>假假</td></tr>
</table>
<ul>
  <li>效率: 真真 / (真真 + 假真)</li>
  <li>误判率: 真假 / (真假 + 假假)</li>
  <li>方差:
    <ul>
      <li>\(V(&epsilon;_{SS}) = \frac{&epsilon;_{SS} (1 - &epsilon;_{SS})}{N_{S}}\)</li>
      <li>\(V(r)/r^{2} = V(&epsilon;_{SS}) / &epsilon;_{SS}^{2} + V(&epsilon;_{SB}) / &epsilon;_{SB}^{2}\)</li>
    </ul>
  </li>
</ul>
<h2>方法</h2>
<ul>
  <li>贝叶斯决策</li>
  <li>线性判别</li>
  <li>决策树方法</li>
  <li>人工神经网络</li>
  <li>近邻法</li>
  <li>概率密度估计量方法</li>
  <li>H 矩阵判别</li>
  <li>函数判别</li>
  <li>SVM</li>
</ul>
<h1>小信号测量的区间估计</h1>
<h1>测量误差及其分类</h1>
<details><summary>和子样误差放在一起</summary>
<p>课上把测量误差和子样误差分开来讲了, 实际上我觉得这两个东西应当放在一起讲,
  毕竟这两个才是真的相关的知识, 也是在实验里真的用的东西.</p>
<p>明明可以用一个实验数据的采集的全流程来讲随机子样, 子样分布函数这些知识,
  非要用概念朗诵, 鼠标扫扫公式 &#8211; 大家要记住哦, 很重要哦, 这种方式来教课&#8230;</p>
<p>不过也可以理解就是了 &#8211; who cares?</p>
</details>
<p><b>Application</b>:</p>
<ul>
  <li>误差传递公式</li>
  <li>探测器效率计算: 实际计数 / 源计数
    <p><b>Application</b>:</p>
    <ul>
      <li>多层符合探测效率
        <ul>
          <li>m 层中至少 k 触发: \(&epsilon;_{\mathrm{tol}} = &sum; \left(\begin{matrix}m &#92;&#92; k\end{matrix}\right) &epsilon;^{i} (1 - &epsilon;)^{m-i}\)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

  </div><a class="u-url" href="/notes/stastics/" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">My Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">My Blog</li><li><a class="u-email" href="mailto:thebigbigwordl@qq.com">thebigbigwordl@qq.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/li-yiyang"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">li-yiyang</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>某不知名的很硬的双非学校的物理系学生的无聊博客</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
